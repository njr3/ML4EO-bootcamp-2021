{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51fe0cb",
   "metadata": {},
   "source": [
    "# Training Data from Radiant MLHub\n",
    "\n",
    "<a style=\"display: inline-block;\" href=\"https://mybinder.org/v2/gh/RadiantMLHub/ml4eo-bootcamp-2021/main?filepath=Lecture%205%2Fexercises%2F4_training_data_from_radiant_mlhub.ipynb\"><img src=\"https://mybinder.org/badge_logo.svg\" alt=\"Launch in Binder\"/></a>\n",
    "\n",
    "Now that we have an understanding of the STAC spec and how it relates to some example training data, we can move on to using the Radiant MLHub API to obtain training datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9c1c6",
   "metadata": {},
   "source": [
    "## Radiant MLHub\n",
    "\n",
    "> Radiant MLHub is an open library for geospatial training data (and soon machine learning models) to advance machine learning applications on Earth Observations. It serves as a resource for a community of practice, giving data scientists benchmarks they can use to train and validate their models and improve its performance.\n",
    ">\n",
    ">  \\- https://mlhub.earth\n",
    "\n",
    "The Radiant MLHub API is a [STAC API-compliant](https://github.com/radiantearth/stac-api-spec) service that enables the search, discovery, and download of geospatial training datasets for use with ML models. MLHub Collections generally fall into 2 categories: source imagery or labels. \n",
    "\n",
    "### Label Collections\n",
    "\n",
    "Label Collections contain assets representing labels that could be used as train or test labels when training a machine learning model. Items in these collections implement the [STAC Label Extension](https://github.com/stac-extensions/label) and may contain vector or raster label assets (or both). Items in a label Collection will have at least one link in their `links` property that refers to an Item from one of the [Source Imagery Collections](#source-imagery-collections) corresponding to the label Item. Label collections may also implement the [STAC Scientific Extension](https://github.com/stac-extensions/scientific) to describe publications associated with the data and how they should be cited.\n",
    "\n",
    "### Source Imagery Collections\n",
    "\n",
    "Source imagery Collections contain assets representing source imagery that can be used in conjuction with labels from a label Collection to train a machine learning model. As described in the [Label Collections](#label-collections) section above, each label Item will have one or more links to a source imagery Item associated with the labels.\n",
    "\n",
    "### Datasets\n",
    "\n",
    "The Radiant MLHub API also defines \"dataset\" objects that collect associated source imagery and label collections into a single object. These \"dataset\" objects ar *not* part of the STAC or STAC API spec. For instance, the `bigearthnet_v1` dataset is comprised of 1 source imagery Collection (`bigearthnet_v1_source`) and 1 label Collection (`bigearthnet_v1_labels`). A request to `GET /datasets/bigearthnet_v1` returns the following:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"collections\": [\n",
    "        {\n",
    "            \"id\": \"bigearthnet_v1_source\",\n",
    "            \"types\": [\n",
    "                \"source_imagery\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"bigearthnet_v1_labels\",\n",
    "            \"types\": [\n",
    "                \"labels\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"id\": \"bigearthnet_v1\",\n",
    "    \"title\": \"BigEarthNet\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Collection Archives\n",
    "\n",
    "While it is possible to crawl all of the STAC Items associated with a Collection and download assets individually, the more typical use-case is to download a Collection or dataset in its entirety. To make it easier to download all of the assets and STAC objects for a given Collection, Radiant MLHub provides a single [tarball archive](https://en.wikipedia.org/wiki/Tar_(computing)) download of most Collections. Collection archives can be downloaded from the `GET /archive/{collection_id}` endpoint.\n",
    "\n",
    "Check out [this blog post](https://medium.com/radiant-earth-insights/archived-training-dataset-downloads-now-available-on-radiant-mlhub-7eb67daf094e) for a more detailed description of the collection archives and how to work with them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3235c1eb",
   "metadata": {},
   "source": [
    "## `radiant-mlhub` Python Client\n",
    "\n",
    "To make it easier to work with the MLHub API in Python code, Radiant Earth Foundation has released an open-source [Python client](https://radiant-mlhub.readthedocs.io/en/latest/) (`radiant-mlhub`). This client may be installed [from `PyPi`](https://pypi.org/project/radiant-mlhub/) or using `conda` ([in the `conda-forge` channel](https://anaconda.org/conda-forge/radiant-mlhub)).\n",
    "\n",
    "Some advantages to using the Python client are:\n",
    "\n",
    "* Easy authentication configuration\n",
    "* Convenience methods for making requests using Python\n",
    "* Ability to work with responses as [PySTAC](https://pystac.readthedocs.io/en/latest/) objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65228764",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "\n",
    "### Create an API Key\n",
    "\n",
    "Access to the Radiant MLHub API requires an API key. To get your API key, go to [dashboard.mlhub.earth](https://dashboard.mlhub.earth). If you have not used Radiant MLHub before, you will need to sign up and create a new account. Otherwise, sign in. In the **API Keys** tab, you'll be able to create API key(s), which you will need. *Do not share* your API key with others: your usage may be limited and sharing your API key is a security risk.\n",
    "\n",
    "### Configure the Client\n",
    "\n",
    "Once you have your API key, you need to configure the `radiant_mlhub` library to use that key. There are a number of ways to configure this (see the [Authentication docs](https://radiant-mlhub.readthedocs.io/en/latest/authentication.html) for details). \n",
    "\n",
    "For these examples, we will set the `MLHUB_API_KEY` environment variable. Run the cell below to save your API key as an environment variable that the client library will recognize.\n",
    "\n",
    "*If you are running this notebook locally and have configured a profile as described in the [Authentication docs](https://radiant-mlhub.readthedocs.io/en/latest/authentication.html), then you do not need to execute this cell.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9013ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MLHUB_API_KEY'] = 'PASTE_YOUR_API_KEY_HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b20d3",
   "metadata": {},
   "source": [
    "## Find a Dataset\n",
    "\n",
    "In this exercise, we will use the `radiant_mlhub` client to find a dataset of interest and download the collection archives for that dataset. The [`radiant_mlhub.Dataset`](https://radiant-mlhub.readthedocs.io/en/latest/api/radiant_mlhub.html#radiant_mlhub.models.Dataset) class has some convenient methods for working with datasets and their associated collections.\n",
    "\n",
    "First, let's import the libraries we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd8cc6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tarfile\n",
    "\n",
    "from radiant_mlhub import Dataset\n",
    "\n",
    "tmp_dir = Path.cwd().parent / 'tmp'\n",
    "tmp_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d83f3",
   "metadata": {},
   "source": [
    "We can use the [`Dataset.list`](https://radiant-mlhub.readthedocs.io/en/latest/api/radiant_mlhub.html#radiant_mlhub.models.Dataset.list) method to get a [Python generator](https://realpython.com/introduction-to-python-generators/) of all the Radiant MLHub datasets. We will then loop through these datasets and print the IDs and titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb4962f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idiv_asia_crop_type                A crop type dataset for consistent land cover classification in Central Asia\n",
      "bigearthnet_v1                     BigEarthNet\n",
      "microsoft_chesapeake               Chesapeake Land Cover\n",
      "ref_african_crops_kenya_02         CV4A Kenya Crop Type Competition\n",
      "ref_african_crops_uganda_01        Dalberg Data Insights Crop Type Uganda\n",
      "rti_rwanda_crop_type               Drone Imagery Classification Training Dataset for Crop Types in Rwanda\n",
      "ref_african_crops_tanzania_01      Great African Food Company Crop Type Tanzania\n",
      "landcovernet_v1                    LandCoverNet\n",
      "open_cities_ai_challenge           Open Cities AI Challenge\n",
      "ref_african_crops_kenya_01         PlantVillage Crop Type Kenya\n",
      "su_african_crops_ghana             Semantic Segmentation of Crop Type in Ghana\n",
      "su_african_crops_south_sudan       Semantic Segmentation of Crop Type in South Sudan\n",
      "sen12floods                        SEN12-FLOOD\n",
      "ts_cashew_benin                    Smallholder Cashew Plantations in Benin\n",
      "spacenet1                          Spacenet 1\n",
      "spacenet2                          Spacenet 2\n",
      "spacenet3                          Spacenet 3\n",
      "spacenet4                          Spacenet 4\n",
      "spacenet5                          Spacenet 5\n",
      "spacenet6                          Spacenet 6\n",
      "spacenet7                          Spacenet 7\n",
      "nasa_tropical_storm_competition    Tropical Cyclone Wind Estimation Competition\n",
      "su_sar_moisture_content_main       Western USA Live Fuel Moisture\n"
     ]
    }
   ],
   "source": [
    "for dataset in Dataset.list():\n",
    "    print(f'{dataset.id: <34} {dataset.title}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b03e01c",
   "metadata": {},
   "source": [
    "For this exercise, let's work with the `su_african_crops_south_sudan` dataset. We can fetch this dataset using the [`Dataset.fetch`](https://radiant-mlhub.readthedocs.io/en/latest/api/radiant_mlhub.html#radiant_mlhub.models.Dataset.fetch) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aa9da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.fetch('su_african_crops_south_sudan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd937e",
   "metadata": {},
   "source": [
    "Once we have the dataset, we can list the collections associated with it. The `Dataset.collections` property is a custom list-like class that allows you to list source imagery collections, label collections, or all collections associated with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c17adc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Collections\n",
      "---------------\n",
      "su_african_crops_south_sudan_labels\n",
      "su_african_crops_south_sudan_source_planet\n",
      "su_african_crops_south_sudan_source_s1\n",
      "su_african_crops_south_sudan_source_s2\n",
      "\n",
      "Source Imagery Collections\n",
      "---------------\n",
      "su_african_crops_south_sudan_source_planet\n",
      "su_african_crops_south_sudan_source_s1\n",
      "su_african_crops_south_sudan_source_s2\n",
      "\n",
      "Label Collections\n",
      "---------------\n",
      "su_african_crops_south_sudan_labels\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all collections\n",
    "print('All Collections\\n---------------')\n",
    "for collection in dataset.collections:\n",
    "    print(collection.id)\n",
    "print('')\n",
    "\n",
    "# List source imagery collections\n",
    "print('Source Imagery Collections\\n---------------')\n",
    "for collection in dataset.collections.source_imagery:\n",
    "    print(collection.id)\n",
    "print('')\n",
    "\n",
    "# List label collections\n",
    "print('Label Collections\\n---------------')\n",
    "for collection in dataset.collections.labels:\n",
    "    print(collection.id)\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b2fd2",
   "metadata": {},
   "source": [
    "We can use the [`Dataset.download`](https://radiant-mlhub.readthedocs.io/en/latest/api/radiant_mlhub.html#radiant_mlhub.models.Dataset.download) method to download all of the collection archives associated with this dataset. This is probably the best approach when working with a dataset because it ensures you have all source imagery and labels for that dataset. However, this can take a significant amount of time due to the size of the archives.\n",
    "\n",
    "For the purposes of this exercise, we will just download the label collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0ba5b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_collection = dataset.collections.labels[0]\n",
    "archive_path = label_collection.download(tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2d8a4d",
   "metadata": {},
   "source": [
    "We can now use the `tarfile` package to unpack the tarball archive into our temporary directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a70252ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the label collection into the directory\n",
    "with tarfile.open(archive_path) as t:\n",
    "    t.extractall(path=tmp_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e61ab528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_common\r\n",
      "collection.json\r\n",
      "su_african_crops_south_sudan_labels_000000\r\n",
      "su_african_crops_south_sudan_labels_000001\r\n",
      "su_african_crops_south_sudan_labels_000002\r\n",
      "su_african_crops_south_sudan_labels_000003\r\n",
      "su_african_crops_south_sudan_labels_000004\r\n",
      "su_african_crops_south_sudan_labels_000005\r\n",
      "su_african_crops_south_sudan_labels_000006\r\n",
      "su_african_crops_south_sudan_labels_000007\r\n",
      "su_african_crops_south_sudan_labels_000008\r\n",
      "su_african_crops_south_sudan_labels_000009\r\n",
      "su_african_crops_south_sudan_labels_000010\r\n",
      "su_african_crops_south_sudan_labels_000011\r\n",
      "su_african_crops_south_sudan_labels_000012\r\n",
      "su_african_crops_south_sudan_labels_000013\r\n",
      "su_african_crops_south_sudan_labels_000014\r\n",
      "su_african_crops_south_sudan_labels_000015\r\n",
      "su_african_crops_south_sudan_labels_000016\r\n",
      "su_african_crops_south_sudan_labels_000017\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../tmp/su_african_crops_south_sudan_labels | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8889bb02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
