{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop detection from satellite imagery using deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook to describe first place solution for ICLR Workshop Challenge #2: Radiant Earth Computer Vision for Crop Detection from Satellite Imagery.\n",
    "You can find the original code here: https://github.com/karimmamer/CropDetectionDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from utils import load_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/karim/Documents/ml4eo2021/ref_african_crops_kenya_01_labels.tar.gz')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from radiant_mlhub import Dataset\n",
    "dataset = Dataset.fetch('ref_african_crops_kenya_02')\n",
    "source_collection = dataset.collections.source_imagery[0]\n",
    "source_collection.download(data_path)\n",
    "lbl_collection = dataset.collections.labels[0]\n",
    "lbl_collection.download(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "def untar_data(fname):\n",
    "    tar = tarfile.open(fname, \"r:gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "untar_data('ref_african_crops_kenya_02_source.tar.gz')\n",
    "untar_data('ref_african_crops_kenya_02_labels.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to download also the sample submission file from here: https://zindi.africa/competitions/iclr-workshop-challenge-2-radiant-earth-computer-vision-for-crop-recognition/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we will prepare the dataset by cropping small patches of 32X32 around each field as the original images are huge and most of its area are not included in training nor test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dates that an observation from Sentinel-2 is provided in the training dataset\n",
    "dates = [datetime.datetime(2019, 6, 6, 8, 10, 7),\n",
    "         datetime.datetime(2019, 7, 1, 8, 10, 4),\n",
    "         datetime.datetime(2019, 7, 6, 8, 10, 8),\n",
    "         datetime.datetime(2019, 7, 11, 8, 10, 4),\n",
    "         datetime.datetime(2019, 7, 21, 8, 10, 4),\n",
    "         datetime.datetime(2019, 8, 5, 8, 10, 7),\n",
    "         datetime.datetime(2019, 8, 15, 8, 10, 6),\n",
    "         datetime.datetime(2019, 8, 25, 8, 10, 4),\n",
    "         datetime.datetime(2019, 9, 9, 8, 9, 58),\n",
    "         datetime.datetime(2019, 9, 19, 8, 9, 59),\n",
    "         datetime.datetime(2019, 9, 24, 8, 9, 59),\n",
    "         datetime.datetime(2019, 10, 4, 8, 10),\n",
    "         datetime.datetime(2019, 11, 3, 8, 10)]\n",
    "\n",
    "# List of bands names included in the dataset\n",
    "bands = ['B01', 'B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B8A', 'B09', 'B11', 'B12', 'CLD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_arr = np.zeros((4,13,13,3035,2016), dtype = np.float32) # 4 tiles X 13 days X 13 bands X 3035 pixels X 2016 pixels\n",
    "#read all images\n",
    "for tile in range(4):\n",
    "    for idx, d in enumerate(dates): # 2) For each date\n",
    "        d = ''.join(str(d.date()).split('-')) # Nice date string\n",
    "        t = '0'+str(tile)\n",
    "        for ibx, b in enumerate(bands): # 3) For each band\n",
    "            # Load im\n",
    "            #im = load_file(f\"{data_path}/{t}/{d}/{t[1]}_{b}_{d}.tif\").astype(np.float32)\n",
    "            im = load_file(f\"{data_path}/ref_african_crops_kenya_02_source/ref_african_crops_kenya_02_tile_{t}_{d}/{b}.tif\").astype(np.float32)\n",
    "            bands_arr[tile,idx,ibx] = im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(bands_arr, radius):\n",
    "    \"\"\"\n",
    "    This a function that go through each field in the data and crop a (2*radius)X(2*radius) patch.\n",
    "    \"\"\"\n",
    "    imgs = np.zeros((4688,13,13,radius*2,radius*2), dtype = np.float32) # patches matrix of dataset_size X 13 days X 13 bands X (2*radius)X(2*radius)\n",
    "    areas = np.zeros((4688,), dtype = np.int) # it has the area of each field in the dataset\n",
    "    gts = np.zeros((4688,), dtype = np.int) # it has the ground truth of each field in the dataset\n",
    "    field_masks = np.zeros((4688,1,radius*2,radius*2), dtype = np.float32) # fields matrix that has ones for the pixel belongs to the field, zeros otherwith of dataset_size X (2*radius)X(2*radius)\n",
    "    fields_arr = []\n",
    "    ifx = 0\n",
    "    \n",
    "    for tile in range(4):\n",
    "        #load field id and label matrices of the tile\n",
    "        fids = f'{data_path}/ref_african_crops_kenya_02_labels/ref_african_crops_kenya_02_tile_0{tile}_label/field_ids.tif'\n",
    "        labs = f'{data_path}/ref_african_crops_kenya_02_labels/ref_african_crops_kenya_02_tile_0{tile}_label/labels.tif'\n",
    "        field_id = load_file(fids)\n",
    "        labels = load_file(labs)\n",
    "        \n",
    "        for field in np.unique(field_id):\n",
    "            if field == 0: # if a pixel has 0 as an ID, it means it is not included neither in training nor test data.\n",
    "              continue\n",
    "            fields_arr.append(field)\n",
    "            \n",
    "            #find pixels belong to current field id\n",
    "            area_mask = field_id == field\n",
    "            \n",
    "            #extract ground-truth class\n",
    "            area_gt = np.unique(labels[area_mask])[0] \n",
    "            \n",
    "            #calculate the median pixel position to crop around it\n",
    "            idxx = np.where(area_mask)\n",
    "            momentx = np.median(idxx[0]).astype(np.int)\n",
    "            momenty = np.median(idxx[1]).astype(np.int)\n",
    "            \n",
    "            #create crop\n",
    "            patch = bands_arr[tile,:,:,max(0, momentx-radius): momentx+radius, max(0, momenty-radius): momenty+radius]\n",
    "            \n",
    "            #pad crops in tiles borders with zeros\n",
    "            imgs[ifx, :, :, :patch.shape[-2], :patch.shape[-1]] = patch\n",
    "            \n",
    "            #create crop's field mask (1s for pixels belong to current field id and zeros otherwise)\n",
    "            field_patch = area_mask[max(0, momentx-radius): momentx+radius, max(0, momenty-radius): momenty+radius]\n",
    "            \n",
    "            #pad crop's field mask in tiles borders with zeros\n",
    "            field_masks[ifx, 0, :patch.shape[-2], :patch.shape[-1]] = field_patch\n",
    "            \n",
    "            #make sure the crop's field mask is not empty\n",
    "            if field_patch.sum() == 0:\n",
    "                print(ifx, momentx-radius, momentx+radius, momenty-radius, momenty+radius)\n",
    "            \n",
    "            #calculate field area\n",
    "            areas[ifx] = area_mask.sum()\n",
    "            gts[ifx] = area_gt - 1\n",
    "            ifx += 1\n",
    "    \n",
    "    return imgs, areas, gts, field_masks, fields_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create crops of 32X32 around each field id center\n",
    "imgs, areas, gts, field_masks, fields_arr = create_dataset(bands_arr, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, counts = np.unique(gts[gts > -1], return_counts=True)\n",
    "labels = ['Maize', 'Cassava', 'Common Beans', 'Maize & Common Beans', 'Maize & Cassava', 'Maize & Soybean', 'Cassava & Common Beans']\n",
    "plt.bar(labels, counts, align='center', width = 0.5)\n",
    "plt.gca().set_xticks(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'No. samples')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "plt.hist(areas[gts > -1], bins=250)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Area (in pixels)')\n",
    "plt.ylabel('No. samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_skewness = []\n",
    "band_skewness_after_sqrt = []\n",
    "for c in range(13):\n",
    "    #calculate skewness\n",
    "    band_skewness.append(pd.Series(imgs[:,:,c].flatten()).skew())\n",
    "    #calculate skewness after applying sqrt\n",
    "    band_skewness_after_sqrt.append(pd.Series(np.sqrt(imgs[:,:,c].flatten())).skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "width = 0.25\n",
    "\n",
    "rng = [i for i in range(0,13)]\n",
    "rng2 = [i+width for i in range(0,13)]\n",
    "\n",
    "plt.bar(rng, band_skewness, align='center', width = width, label='Original', color ='b')\n",
    "plt.bar(rng2, band_skewness_after_sqrt, align='center', width = width, label='After sqrt', color ='g')\n",
    "plt.gca().set_xticks(rng)\n",
    "plt.legend()\n",
    "plt.xlabel('Band ID')\n",
    "plt.ylabel('Skewness')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate vegitation indecies for training and testing data\n",
    "ndvi = (imgs[:,:,7:8,:,:] - imgs[:,:,3:4,:,:]) / (imgs[:,:,7:8,:,:] + imgs[:,:,3:4,:,:] + 1e-6)\n",
    "ndwi_green = (imgs[:,:,2:3,:,:] - imgs[:,:,7:8,:,:]) / (imgs[:,:,2:3,:,:] + imgs[:,:,7:8,:,:] + 1e-6)\n",
    "ndwi_blue = (imgs[:,:,1:2,:,:] - imgs[:,:,7:8,:,:]) / (imgs[:,:,1:2,:,:] + imgs[:,:,7:8,:,:] + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply sqrt to lower skewness\n",
    "imgs = np.sqrt(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.concatenate([imgs, ndvi, ndwi_green, ndwi_blue], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data standardization\n",
    "for c in range(imgs.shape[2]):\n",
    "    mean = imgs[:, :, c].mean()\n",
    "    std = imgs[:, :, c].std()\n",
    "    imgs[:, :, c] = (imgs[:, :, c] - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local validation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from dataset import ICLRDataset\n",
    "from model import ConvGRUNet\n",
    "from utils import test, train_model_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#set all seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Cycle 0: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.4757 Acc: 0.4654\n",
      "val Loss: 1.3832 Acc: 0.5416\n",
      "\n",
      "Cycle 0: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.3736 Acc: 0.5156\n",
      "val Loss: 1.3560 Acc: 0.5497\n",
      "\n",
      "Cycle 0: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.3376 Acc: 0.5313\n",
      "val Loss: 1.3826 Acc: 0.5152\n",
      "\n",
      "Cycle 0: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.3149 Acc: 0.5313\n",
      "val Loss: 1.3509 Acc: 0.5396\n",
      "\n",
      "Cycle 0: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.2937 Acc: 0.5446\n",
      "val Loss: 1.2723 Acc: 0.5619\n",
      "\n",
      "Cycle 0: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.2616 Acc: 0.5589\n",
      "val Loss: 1.2528 Acc: 0.5700\n",
      "\n",
      "Cycle 0: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.2251 Acc: 0.5700\n",
      "val Loss: 1.2100 Acc: 0.5740\n",
      "\n",
      "Cycle 0: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.1989 Acc: 0.5829\n",
      "val Loss: 1.2071 Acc: 0.5822\n",
      "\n",
      "Cycle 0: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.1781 Acc: 0.5850\n",
      "val Loss: 1.2071 Acc: 0.5700\n",
      "\n",
      "Cycle 0: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.1759 Acc: 0.5908\n",
      "val Loss: 1.2072 Acc: 0.5680\n",
      "\n",
      "Cycle 1: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.2775 Acc: 0.5517\n",
      "val Loss: 1.2422 Acc: 0.5659\n",
      "\n",
      "Cycle 1: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.2563 Acc: 0.5639\n",
      "val Loss: 1.2450 Acc: 0.5740\n",
      "\n",
      "Cycle 1: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.2272 Acc: 0.5793\n",
      "val Loss: 1.2185 Acc: 0.5862\n",
      "\n",
      "Cycle 1: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.2026 Acc: 0.5854\n",
      "val Loss: 1.2462 Acc: 0.5598\n",
      "\n",
      "Cycle 1: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1774 Acc: 0.5847\n",
      "val Loss: 1.2223 Acc: 0.5822\n",
      "\n",
      "Cycle 1: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.1544 Acc: 0.5915\n",
      "val Loss: 1.1645 Acc: 0.5882\n",
      "\n",
      "Cycle 1: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.1255 Acc: 0.6119\n",
      "val Loss: 1.1519 Acc: 0.5903\n",
      "\n",
      "Cycle 1: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.1037 Acc: 0.6205\n",
      "val Loss: 1.1615 Acc: 0.5761\n",
      "\n",
      "Cycle 1: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0840 Acc: 0.6244\n",
      "val Loss: 1.1557 Acc: 0.5801\n",
      "\n",
      "Cycle 1: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0728 Acc: 0.6230\n",
      "val Loss: 1.1533 Acc: 0.5842\n",
      "\n",
      "Cycle 2: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.2107 Acc: 0.5768\n",
      "val Loss: 1.2008 Acc: 0.5700\n",
      "\n",
      "Cycle 2: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1721 Acc: 0.5865\n",
      "val Loss: 1.2675 Acc: 0.5497\n",
      "\n",
      "Cycle 2: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1606 Acc: 0.6037\n",
      "val Loss: 1.1827 Acc: 0.5720\n",
      "\n",
      "Cycle 2: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1461 Acc: 0.5983\n",
      "val Loss: 1.1843 Acc: 0.5700\n",
      "\n",
      "Cycle 2: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1089 Acc: 0.6165\n",
      "val Loss: 1.1853 Acc: 0.5659\n",
      "\n",
      "Cycle 2: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.0920 Acc: 0.6237\n",
      "val Loss: 1.1396 Acc: 0.5882\n",
      "\n",
      "Cycle 2: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0741 Acc: 0.6291\n",
      "val Loss: 1.1874 Acc: 0.5822\n",
      "\n",
      "Cycle 2: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.0543 Acc: 0.6355\n",
      "val Loss: 1.1516 Acc: 0.5842\n",
      "\n",
      "Cycle 2: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0232 Acc: 0.6470\n",
      "val Loss: 1.1379 Acc: 0.5903\n",
      "\n",
      "Cycle 2: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0184 Acc: 0.6488\n",
      "val Loss: 1.1395 Acc: 0.5903\n",
      "\n",
      "Cycle 3: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1585 Acc: 0.5926\n",
      "val Loss: 1.2072 Acc: 0.5517\n",
      "\n",
      "Cycle 3: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1406 Acc: 0.6069\n",
      "val Loss: 1.2283 Acc: 0.5659\n",
      "\n",
      "Cycle 3: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1013 Acc: 0.6144\n",
      "val Loss: 1.1751 Acc: 0.5720\n",
      "\n",
      "Cycle 3: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0871 Acc: 0.6226\n",
      "val Loss: 1.1710 Acc: 0.5842\n",
      "\n",
      "Cycle 3: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0624 Acc: 0.6198\n",
      "val Loss: 1.1645 Acc: 0.5761\n",
      "\n",
      "Cycle 3: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.0459 Acc: 0.6330\n",
      "val Loss: 1.1879 Acc: 0.5720\n",
      "\n",
      "Cycle 3: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0299 Acc: 0.6352\n",
      "val Loss: 1.1505 Acc: 0.5822\n",
      "\n",
      "Cycle 3: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9992 Acc: 0.6516\n",
      "val Loss: 1.1401 Acc: 0.5862\n",
      "\n",
      "Cycle 3: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9740 Acc: 0.6552\n",
      "val Loss: 1.1446 Acc: 0.5903\n",
      "\n",
      "Cycle 3: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.9641 Acc: 0.6634\n",
      "val Loss: 1.1429 Acc: 0.5923\n",
      "\n",
      "Cycle 4: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1077 Acc: 0.6151\n",
      "val Loss: 1.2161 Acc: 0.5700\n",
      "\n",
      "Cycle 4: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0809 Acc: 0.6208\n",
      "val Loss: 1.2148 Acc: 0.5619\n",
      "\n",
      "Cycle 4: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0693 Acc: 0.6158\n",
      "val Loss: 1.1707 Acc: 0.6126\n",
      "\n",
      "Cycle 4: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0486 Acc: 0.6309\n",
      "val Loss: 1.1656 Acc: 0.5801\n",
      "\n",
      "Cycle 4: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0214 Acc: 0.6387\n",
      "val Loss: 1.1741 Acc: 0.5923\n",
      "\n",
      "Cycle 4: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9751 Acc: 0.6484\n",
      "val Loss: 1.1821 Acc: 0.5781\n",
      "\n",
      "Cycle 4: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.9575 Acc: 0.6599\n",
      "val Loss: 1.1707 Acc: 0.5943\n",
      "\n",
      "Cycle 4: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9300 Acc: 0.6602\n",
      "val Loss: 1.1575 Acc: 0.5963\n",
      "\n",
      "Cycle 4: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9029 Acc: 0.6710\n",
      "val Loss: 1.1540 Acc: 0.6004\n",
      "\n",
      "Cycle 4: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.8885 Acc: 0.6774\n",
      "val Loss: 1.1521 Acc: 0.5903\n",
      "\n",
      "Cycle 5: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.0432 Acc: 0.6301\n",
      "val Loss: 1.2003 Acc: 0.5822\n",
      "\n",
      "Cycle 5: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0395 Acc: 0.6327\n",
      "val Loss: 1.1611 Acc: 0.6024\n",
      "\n",
      "Cycle 5: Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.9992 Acc: 0.6362\n",
      "val Loss: 1.2150 Acc: 0.5680\n",
      "\n",
      "Cycle 5: Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.9921 Acc: 0.6441\n",
      "val Loss: 1.1920 Acc: 0.5781\n",
      "\n",
      "Cycle 5: Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.9415 Acc: 0.6584\n",
      "val Loss: 1.2092 Acc: 0.5943\n",
      "\n",
      "Cycle 5: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9104 Acc: 0.6745\n",
      "val Loss: 1.1798 Acc: 0.5882\n",
      "\n",
      "Cycle 5: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.8887 Acc: 0.6831\n",
      "val Loss: 1.1821 Acc: 0.5781\n",
      "\n",
      "Cycle 5: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.8296 Acc: 0.7014\n",
      "val Loss: 1.1946 Acc: 0.5740\n",
      "\n",
      "Cycle 5: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.8007 Acc: 0.7150\n",
      "val Loss: 1.1844 Acc: 0.5862\n",
      "\n",
      "Cycle 5: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.7926 Acc: 0.7114\n",
      "val Loss: 1.1797 Acc: 0.5801\n",
      "\n",
      "Training complete in 9m 34s\n",
      "Ensemble Loss : 1.125480, Best val Loss: 1.137872\n",
      "1\n",
      "Cycle 0: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.4711 Acc: 0.4898\n",
      "val Loss: 1.3931 Acc: 0.4929\n",
      "\n",
      "Cycle 0: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.3689 Acc: 0.5238\n",
      "val Loss: 1.4029 Acc: 0.4949\n",
      "\n",
      "Cycle 0: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.3417 Acc: 0.5274\n",
      "val Loss: 1.3177 Acc: 0.5355\n",
      "\n",
      "Cycle 0: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.3037 Acc: 0.5414\n",
      "val Loss: 1.4054 Acc: 0.5030\n",
      "\n",
      "Cycle 0: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.2822 Acc: 0.5550\n",
      "val Loss: 1.2875 Acc: 0.5477\n",
      "\n",
      "Cycle 0: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.2472 Acc: 0.5578\n",
      "val Loss: 1.2481 Acc: 0.5517\n",
      "\n",
      "Cycle 0: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.2259 Acc: 0.5732\n",
      "val Loss: 1.2131 Acc: 0.5740\n",
      "\n",
      "Cycle 0: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.2064 Acc: 0.5750\n",
      "val Loss: 1.2025 Acc: 0.5923\n",
      "\n",
      "Cycle 0: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.1772 Acc: 0.5926\n",
      "val Loss: 1.1998 Acc: 0.5882\n",
      "\n",
      "Cycle 0: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.1653 Acc: 0.5954\n",
      "val Loss: 1.1932 Acc: 0.5822\n",
      "\n",
      "Cycle 1: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.2795 Acc: 0.5557\n",
      "val Loss: 1.2582 Acc: 0.5659\n",
      "\n",
      "Cycle 1: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.2447 Acc: 0.5625\n",
      "val Loss: 1.2399 Acc: 0.5740\n",
      "\n",
      "Cycle 1: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.2224 Acc: 0.5757\n",
      "val Loss: 1.1982 Acc: 0.5700\n",
      "\n",
      "Cycle 1: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1925 Acc: 0.5858\n",
      "val Loss: 1.2049 Acc: 0.5740\n",
      "\n",
      "Cycle 1: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1883 Acc: 0.5793\n",
      "val Loss: 1.1978 Acc: 0.5740\n",
      "\n",
      "Cycle 1: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.1457 Acc: 0.5961\n",
      "val Loss: 1.1405 Acc: 0.6085\n",
      "\n",
      "Cycle 1: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.1352 Acc: 0.6047\n",
      "val Loss: 1.1244 Acc: 0.6065\n",
      "\n",
      "Cycle 1: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.1056 Acc: 0.6130\n",
      "val Loss: 1.1233 Acc: 0.6207\n",
      "\n",
      "Cycle 1: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0955 Acc: 0.6205\n",
      "val Loss: 1.1175 Acc: 0.6166\n",
      "\n",
      "Cycle 1: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0839 Acc: 0.6119\n",
      "val Loss: 1.1158 Acc: 0.6207\n",
      "\n",
      "Cycle 2: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.2086 Acc: 0.5754\n",
      "val Loss: 1.2438 Acc: 0.5578\n",
      "\n",
      "Cycle 2: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1884 Acc: 0.5840\n",
      "val Loss: 1.2379 Acc: 0.5578\n",
      "\n",
      "Cycle 2: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1834 Acc: 0.5843\n",
      "val Loss: 1.1723 Acc: 0.5903\n",
      "\n",
      "Cycle 2: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1516 Acc: 0.5879\n",
      "val Loss: 1.2105 Acc: 0.5740\n",
      "\n",
      "Cycle 2: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1360 Acc: 0.6051\n",
      "val Loss: 1.1408 Acc: 0.6004\n",
      "\n",
      "Cycle 2: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.1052 Acc: 0.6137\n",
      "val Loss: 1.1232 Acc: 0.6065\n",
      "\n",
      "Cycle 2: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0888 Acc: 0.6137\n",
      "val Loss: 1.1142 Acc: 0.5943\n",
      "\n",
      "Cycle 2: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.0541 Acc: 0.6248\n",
      "val Loss: 1.0910 Acc: 0.6126\n",
      "\n",
      "Cycle 2: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0453 Acc: 0.6287\n",
      "val Loss: 1.0920 Acc: 0.6126\n",
      "\n",
      "Cycle 2: Epoch 9/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0412 Acc: 0.6334\n",
      "val Loss: 1.0904 Acc: 0.6085\n",
      "\n",
      "Cycle 3: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1483 Acc: 0.5965\n",
      "val Loss: 1.1482 Acc: 0.6105\n",
      "\n",
      "Cycle 3: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1341 Acc: 0.5933\n",
      "val Loss: 1.1995 Acc: 0.5882\n",
      "\n",
      "Cycle 3: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1285 Acc: 0.6076\n",
      "val Loss: 1.1609 Acc: 0.5822\n",
      "\n",
      "Cycle 3: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1068 Acc: 0.6108\n",
      "val Loss: 1.1547 Acc: 0.5903\n",
      "\n",
      "Cycle 3: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0719 Acc: 0.6273\n",
      "val Loss: 1.1142 Acc: 0.5943\n",
      "\n",
      "Cycle 3: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.0584 Acc: 0.6269\n",
      "val Loss: 1.1243 Acc: 0.5984\n",
      "\n",
      "Cycle 3: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0170 Acc: 0.6377\n",
      "val Loss: 1.1104 Acc: 0.6268\n",
      "\n",
      "Cycle 3: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.0094 Acc: 0.6409\n",
      "val Loss: 1.0896 Acc: 0.6187\n",
      "\n",
      "Cycle 3: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9979 Acc: 0.6480\n",
      "val Loss: 1.0854 Acc: 0.6227\n",
      "\n",
      "Cycle 3: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.9797 Acc: 0.6495\n",
      "val Loss: 1.0853 Acc: 0.6247\n",
      "\n",
      "Cycle 4: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1205 Acc: 0.6108\n",
      "val Loss: 1.1782 Acc: 0.5923\n",
      "\n",
      "Cycle 4: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0926 Acc: 0.6201\n",
      "val Loss: 1.1500 Acc: 0.5801\n",
      "\n",
      "Cycle 4: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0943 Acc: 0.6115\n",
      "val Loss: 1.1442 Acc: 0.6004\n",
      "\n",
      "Cycle 4: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0562 Acc: 0.6230\n",
      "val Loss: 1.1332 Acc: 0.6065\n",
      "\n",
      "Cycle 4: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0353 Acc: 0.6319\n",
      "val Loss: 1.1037 Acc: 0.6004\n",
      "\n",
      "Cycle 4: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.0071 Acc: 0.6301\n",
      "val Loss: 1.1018 Acc: 0.6146\n",
      "\n",
      "Cycle 4: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.9719 Acc: 0.6570\n",
      "val Loss: 1.1066 Acc: 0.6004\n",
      "\n",
      "Cycle 4: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9455 Acc: 0.6663\n",
      "val Loss: 1.0864 Acc: 0.6207\n",
      "\n",
      "Cycle 4: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9398 Acc: 0.6663\n",
      "val Loss: 1.0783 Acc: 0.6146\n",
      "\n",
      "Cycle 4: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.9253 Acc: 0.6692\n",
      "val Loss: 1.0784 Acc: 0.6105\n",
      "\n",
      "Cycle 5: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.0758 Acc: 0.6158\n",
      "val Loss: 1.1727 Acc: 0.5781\n",
      "\n",
      "Cycle 5: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0530 Acc: 0.6223\n",
      "val Loss: 1.2076 Acc: 0.5761\n",
      "\n",
      "Cycle 5: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0295 Acc: 0.6352\n",
      "val Loss: 1.1958 Acc: 0.5720\n",
      "\n",
      "Cycle 5: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0010 Acc: 0.6448\n",
      "val Loss: 1.2261 Acc: 0.5639\n",
      "\n",
      "Cycle 5: Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.9841 Acc: 0.6427\n",
      "val Loss: 1.1468 Acc: 0.6126\n",
      "\n",
      "Cycle 5: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9552 Acc: 0.6599\n",
      "val Loss: 1.1162 Acc: 0.5903\n",
      "\n",
      "Cycle 5: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.9150 Acc: 0.6742\n",
      "val Loss: 1.0946 Acc: 0.6065\n",
      "\n",
      "Cycle 5: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.8895 Acc: 0.6831\n",
      "val Loss: 1.0920 Acc: 0.6065\n",
      "\n",
      "Cycle 5: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.8709 Acc: 0.6853\n",
      "val Loss: 1.0908 Acc: 0.6146\n",
      "\n",
      "Cycle 5: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.8469 Acc: 0.6903\n",
      "val Loss: 1.0909 Acc: 0.6187\n",
      "\n",
      "Training complete in 9m 53s\n",
      "Ensemble Loss : 1.077088, Best val Loss: 1.078339\n",
      "2\n",
      "Cycle 0: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.4783 Acc: 0.4701\n",
      "val Loss: 1.3878 Acc: 0.5254\n",
      "\n",
      "Cycle 0: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.3873 Acc: 0.5109\n",
      "val Loss: 1.3739 Acc: 0.4929\n",
      "\n",
      "Cycle 0: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.3455 Acc: 0.5249\n",
      "val Loss: 1.3340 Acc: 0.5355\n",
      "\n",
      "Cycle 0: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.3105 Acc: 0.5442\n",
      "val Loss: 1.2930 Acc: 0.5538\n",
      "\n",
      "Cycle 0: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.2857 Acc: 0.5517\n",
      "val Loss: 1.3006 Acc: 0.5538\n",
      "\n",
      "Cycle 0: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.2451 Acc: 0.5646\n",
      "val Loss: 1.2680 Acc: 0.5619\n",
      "\n",
      "Cycle 0: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.2301 Acc: 0.5693\n",
      "val Loss: 1.2400 Acc: 0.5740\n",
      "\n",
      "Cycle 0: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.1977 Acc: 0.5782\n",
      "val Loss: 1.2277 Acc: 0.5659\n",
      "\n",
      "Cycle 0: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.1786 Acc: 0.5825\n",
      "val Loss: 1.2132 Acc: 0.5700\n",
      "\n",
      "Cycle 0: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.1698 Acc: 0.5883\n",
      "val Loss: 1.2130 Acc: 0.5720\n",
      "\n",
      "Cycle 1: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.2717 Acc: 0.5639\n",
      "val Loss: 1.2750 Acc: 0.5639\n",
      "\n",
      "Cycle 1: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.2528 Acc: 0.5610\n",
      "val Loss: 1.2730 Acc: 0.5274\n",
      "\n",
      "Cycle 1: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.2373 Acc: 0.5682\n",
      "val Loss: 1.2418 Acc: 0.5517\n",
      "\n",
      "Cycle 1: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1991 Acc: 0.5789\n",
      "val Loss: 1.2257 Acc: 0.5700\n",
      "\n",
      "Cycle 1: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1822 Acc: 0.5922\n",
      "val Loss: 1.2096 Acc: 0.5740\n",
      "\n",
      "Cycle 1: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.1556 Acc: 0.5968\n",
      "val Loss: 1.1886 Acc: 0.5801\n",
      "\n",
      "Cycle 1: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.1274 Acc: 0.6019\n",
      "val Loss: 1.1814 Acc: 0.5822\n",
      "\n",
      "Cycle 1: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.1161 Acc: 0.6148\n",
      "val Loss: 1.1773 Acc: 0.5882\n",
      "\n",
      "Cycle 1: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0965 Acc: 0.6155\n",
      "val Loss: 1.1731 Acc: 0.5862\n",
      "\n",
      "Cycle 1: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0848 Acc: 0.6223\n",
      "val Loss: 1.1751 Acc: 0.5842\n",
      "\n",
      "Cycle 2: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1848 Acc: 0.5847\n",
      "val Loss: 1.2582 Acc: 0.5619\n",
      "\n",
      "Cycle 2: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1811 Acc: 0.5865\n",
      "val Loss: 1.1989 Acc: 0.5822\n",
      "\n",
      "Cycle 2: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1704 Acc: 0.5897\n",
      "val Loss: 1.1947 Acc: 0.5862\n",
      "\n",
      "Cycle 2: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1512 Acc: 0.6051\n",
      "val Loss: 1.2080 Acc: 0.5619\n",
      "\n",
      "Cycle 2: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1202 Acc: 0.6001\n",
      "val Loss: 1.2089 Acc: 0.5822\n",
      "\n",
      "Cycle 2: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.1054 Acc: 0.6119\n",
      "val Loss: 1.1827 Acc: 0.5903\n",
      "\n",
      "Cycle 2: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0737 Acc: 0.6183\n",
      "val Loss: 1.1752 Acc: 0.5781\n",
      "\n",
      "Cycle 2: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.0502 Acc: 0.6384\n",
      "val Loss: 1.1709 Acc: 0.5822\n",
      "\n",
      "Cycle 2: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0331 Acc: 0.6377\n",
      "val Loss: 1.1602 Acc: 0.5903\n",
      "\n",
      "Cycle 2: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0308 Acc: 0.6398\n",
      "val Loss: 1.1594 Acc: 0.5943\n",
      "\n",
      "Cycle 3: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1455 Acc: 0.6008\n",
      "val Loss: 1.2993 Acc: 0.5355\n",
      "\n",
      "Cycle 3: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1357 Acc: 0.6047\n",
      "val Loss: 1.2596 Acc: 0.5497\n",
      "\n",
      "Cycle 3: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1247 Acc: 0.6044\n",
      "val Loss: 1.1706 Acc: 0.5720\n",
      "\n",
      "Cycle 3: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1080 Acc: 0.6148\n",
      "val Loss: 1.1785 Acc: 0.5801\n",
      "\n",
      "Cycle 3: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0789 Acc: 0.6155\n",
      "val Loss: 1.1878 Acc: 0.5700\n",
      "\n",
      "Cycle 3: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.0428 Acc: 0.6316\n",
      "val Loss: 1.1756 Acc: 0.5923\n",
      "\n",
      "Cycle 3: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0207 Acc: 0.6355\n",
      "val Loss: 1.1589 Acc: 0.5862\n",
      "\n",
      "Cycle 3: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9975 Acc: 0.6430\n",
      "val Loss: 1.1657 Acc: 0.5903\n",
      "\n",
      "Cycle 3: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9791 Acc: 0.6488\n",
      "val Loss: 1.1703 Acc: 0.5822\n",
      "\n",
      "Cycle 3: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.9722 Acc: 0.6502\n",
      "val Loss: 1.1694 Acc: 0.5781\n",
      "\n",
      "Cycle 4: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1007 Acc: 0.6169\n",
      "val Loss: 1.2442 Acc: 0.5619\n",
      "\n",
      "Cycle 4: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1050 Acc: 0.6112\n",
      "val Loss: 1.1862 Acc: 0.5842\n",
      "\n",
      "Cycle 4: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0686 Acc: 0.6276\n",
      "val Loss: 1.1926 Acc: 0.5720\n",
      "\n",
      "Cycle 4: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0407 Acc: 0.6284\n",
      "val Loss: 1.2081 Acc: 0.5578\n",
      "\n",
      "Cycle 4: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0197 Acc: 0.6362\n",
      "val Loss: 1.2013 Acc: 0.5963\n",
      "\n",
      "Cycle 4: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9978 Acc: 0.6473\n",
      "val Loss: 1.1835 Acc: 0.5720\n",
      "\n",
      "Cycle 4: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.9522 Acc: 0.6642\n",
      "val Loss: 1.1897 Acc: 0.5801\n",
      "\n",
      "Cycle 4: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9294 Acc: 0.6713\n",
      "val Loss: 1.1909 Acc: 0.5781\n",
      "\n",
      "Cycle 4: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9153 Acc: 0.6753\n",
      "val Loss: 1.1858 Acc: 0.5781\n",
      "\n",
      "Cycle 4: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.8982 Acc: 0.6792\n",
      "val Loss: 1.1851 Acc: 0.5862\n",
      "\n",
      "Cycle 5: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.0582 Acc: 0.6119\n",
      "val Loss: 1.2698 Acc: 0.5538\n",
      "\n",
      "Cycle 5: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0468 Acc: 0.6309\n",
      "val Loss: 1.2638 Acc: 0.5477\n",
      "\n",
      "Cycle 5: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0114 Acc: 0.6402\n",
      "val Loss: 1.2270 Acc: 0.5822\n",
      "\n",
      "Cycle 5: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0041 Acc: 0.6369\n",
      "val Loss: 1.2035 Acc: 0.5923\n",
      "\n",
      "Cycle 5: Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.9479 Acc: 0.6599\n",
      "val Loss: 1.2404 Acc: 0.5903\n",
      "\n",
      "Cycle 5: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9315 Acc: 0.6706\n",
      "val Loss: 1.1933 Acc: 0.5700\n",
      "\n",
      "Cycle 5: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.8800 Acc: 0.6817\n",
      "val Loss: 1.2207 Acc: 0.5882\n",
      "\n",
      "Cycle 5: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.8519 Acc: 0.6939\n",
      "val Loss: 1.2013 Acc: 0.5882\n",
      "\n",
      "Cycle 5: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.8391 Acc: 0.6967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2129 Acc: 0.5700\n",
      "\n",
      "Cycle 5: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.8254 Acc: 0.7114\n",
      "val Loss: 1.2144 Acc: 0.5740\n",
      "\n",
      "Training complete in 9m 30s\n",
      "Ensemble Loss : 1.145610, Best val Loss: 1.158862\n",
      "3\n",
      "Cycle 0: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.4946 Acc: 0.4608\n",
      "val Loss: 1.3850 Acc: 0.5193\n",
      "\n",
      "Cycle 0: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.4024 Acc: 0.5120\n",
      "val Loss: 1.3466 Acc: 0.5456\n",
      "\n",
      "Cycle 0: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.3641 Acc: 0.5270\n",
      "val Loss: 1.2863 Acc: 0.5538\n",
      "\n",
      "Cycle 0: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.3421 Acc: 0.5277\n",
      "val Loss: 1.2821 Acc: 0.5517\n",
      "\n",
      "Cycle 0: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.3105 Acc: 0.5346\n",
      "val Loss: 1.2729 Acc: 0.5456\n",
      "\n",
      "Cycle 0: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.2835 Acc: 0.5478\n",
      "val Loss: 1.3145 Acc: 0.5416\n",
      "\n",
      "Cycle 0: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.2641 Acc: 0.5539\n",
      "val Loss: 1.2246 Acc: 0.5862\n",
      "\n",
      "Cycle 0: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.2273 Acc: 0.5696\n",
      "val Loss: 1.2095 Acc: 0.5740\n",
      "\n",
      "Cycle 0: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.2110 Acc: 0.5696\n",
      "val Loss: 1.1970 Acc: 0.5822\n",
      "\n",
      "Cycle 0: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.2026 Acc: 0.5797\n",
      "val Loss: 1.1918 Acc: 0.5842\n",
      "\n",
      "Cycle 1: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.2986 Acc: 0.5496\n",
      "val Loss: 1.2862 Acc: 0.5497\n",
      "\n",
      "Cycle 1: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.2693 Acc: 0.5550\n",
      "val Loss: 1.2483 Acc: 0.5740\n",
      "\n",
      "Cycle 1: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.2500 Acc: 0.5636\n",
      "val Loss: 1.1884 Acc: 0.6004\n",
      "\n",
      "Cycle 1: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.2128 Acc: 0.5736\n",
      "val Loss: 1.1974 Acc: 0.5720\n",
      "\n",
      "Cycle 1: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.2007 Acc: 0.5764\n",
      "val Loss: 1.1835 Acc: 0.5963\n",
      "\n",
      "Cycle 1: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.1756 Acc: 0.5850\n",
      "val Loss: 1.1521 Acc: 0.5984\n",
      "\n",
      "Cycle 1: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.1437 Acc: 0.6008\n",
      "val Loss: 1.1511 Acc: 0.6024\n",
      "\n",
      "Cycle 1: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.1351 Acc: 0.5976\n",
      "val Loss: 1.1441 Acc: 0.6146\n",
      "\n",
      "Cycle 1: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0939 Acc: 0.6151\n",
      "val Loss: 1.1261 Acc: 0.6187\n",
      "\n",
      "Cycle 1: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0929 Acc: 0.6137\n",
      "val Loss: 1.1273 Acc: 0.6146\n",
      "\n",
      "Cycle 2: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1956 Acc: 0.5858\n",
      "val Loss: 1.3900 Acc: 0.5254\n",
      "\n",
      "Cycle 2: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.2081 Acc: 0.5739\n",
      "val Loss: 1.2001 Acc: 0.5882\n",
      "\n",
      "Cycle 2: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1805 Acc: 0.5883\n",
      "val Loss: 1.1569 Acc: 0.6024\n",
      "\n",
      "Cycle 2: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1445 Acc: 0.5886\n",
      "val Loss: 1.1835 Acc: 0.6085\n",
      "\n",
      "Cycle 2: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1352 Acc: 0.5997\n",
      "val Loss: 1.1651 Acc: 0.6105\n",
      "\n",
      "Cycle 2: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.1058 Acc: 0.6097\n",
      "val Loss: 1.1602 Acc: 0.5903\n",
      "\n",
      "Cycle 2: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0792 Acc: 0.6198\n",
      "val Loss: 1.1600 Acc: 0.5984\n",
      "\n",
      "Cycle 2: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.0539 Acc: 0.6194\n",
      "val Loss: 1.1310 Acc: 0.6146\n",
      "\n",
      "Cycle 2: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0318 Acc: 0.6301\n",
      "val Loss: 1.1192 Acc: 0.6187\n",
      "\n",
      "Cycle 2: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0391 Acc: 0.6241\n",
      "val Loss: 1.1168 Acc: 0.6166\n",
      "\n",
      "Cycle 3: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1652 Acc: 0.5854\n",
      "val Loss: 1.1474 Acc: 0.6166\n",
      "\n",
      "Cycle 3: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1411 Acc: 0.5947\n",
      "val Loss: 1.1868 Acc: 0.6024\n",
      "\n",
      "Cycle 3: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1169 Acc: 0.6101\n",
      "val Loss: 1.2050 Acc: 0.5862\n",
      "\n",
      "Cycle 3: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1023 Acc: 0.6126\n",
      "val Loss: 1.1704 Acc: 0.6024\n",
      "\n",
      "Cycle 3: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0712 Acc: 0.6198\n",
      "val Loss: 1.1434 Acc: 0.6065\n",
      "\n",
      "Cycle 3: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.0492 Acc: 0.6223\n",
      "val Loss: 1.1388 Acc: 0.6166\n",
      "\n",
      "Cycle 3: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0302 Acc: 0.6377\n",
      "val Loss: 1.1406 Acc: 0.5842\n",
      "\n",
      "Cycle 3: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9926 Acc: 0.6438\n",
      "val Loss: 1.1326 Acc: 0.6105\n",
      "\n",
      "Cycle 3: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9884 Acc: 0.6463\n",
      "val Loss: 1.1365 Acc: 0.6207\n",
      "\n",
      "Cycle 3: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.9623 Acc: 0.6502\n",
      "val Loss: 1.1354 Acc: 0.6126\n",
      "\n",
      "Cycle 4: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1146 Acc: 0.6144\n",
      "val Loss: 1.1683 Acc: 0.5882\n",
      "\n",
      "Cycle 4: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0853 Acc: 0.6115\n",
      "val Loss: 1.1975 Acc: 0.6166\n",
      "\n",
      "Cycle 4: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0832 Acc: 0.6223\n",
      "val Loss: 1.1407 Acc: 0.6065\n",
      "\n",
      "Cycle 4: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0471 Acc: 0.6287\n",
      "val Loss: 1.2211 Acc: 0.5822\n",
      "\n",
      "Cycle 4: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0259 Acc: 0.6266\n",
      "val Loss: 1.1461 Acc: 0.6085\n",
      "\n",
      "Cycle 4: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9940 Acc: 0.6502\n",
      "val Loss: 1.1668 Acc: 0.6126\n",
      "\n",
      "Cycle 4: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.9652 Acc: 0.6559\n",
      "val Loss: 1.1446 Acc: 0.6146\n",
      "\n",
      "Cycle 4: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9251 Acc: 0.6652\n",
      "val Loss: 1.1526 Acc: 0.6126\n",
      "\n",
      "Cycle 4: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.8941 Acc: 0.6799\n",
      "val Loss: 1.1435 Acc: 0.6105\n",
      "\n",
      "Cycle 4: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.8841 Acc: 0.6806\n",
      "val Loss: 1.1448 Acc: 0.6105\n",
      "\n",
      "Cycle 5: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.0548 Acc: 0.6173\n",
      "val Loss: 1.2284 Acc: 0.5903\n",
      "\n",
      "Cycle 5: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0457 Acc: 0.6269\n",
      "val Loss: 1.2573 Acc: 0.5720\n",
      "\n",
      "Cycle 5: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0123 Acc: 0.6366\n",
      "val Loss: 1.2037 Acc: 0.5822\n",
      "\n",
      "Cycle 5: Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.9898 Acc: 0.6513\n",
      "val Loss: 1.2024 Acc: 0.5761\n",
      "\n",
      "Cycle 5: Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.9612 Acc: 0.6509\n",
      "val Loss: 1.1838 Acc: 0.6105\n",
      "\n",
      "Cycle 5: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9280 Acc: 0.6599\n",
      "val Loss: 1.1954 Acc: 0.5740\n",
      "\n",
      "Cycle 5: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.8851 Acc: 0.6799\n",
      "val Loss: 1.1987 Acc: 0.5862\n",
      "\n",
      "Cycle 5: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.8545 Acc: 0.6960\n",
      "val Loss: 1.1778 Acc: 0.6105\n",
      "\n",
      "Cycle 5: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.8239 Acc: 0.6985\n",
      "val Loss: 1.1784 Acc: 0.6004\n",
      "\n",
      "Cycle 5: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.8083 Acc: 0.7061\n",
      "val Loss: 1.1794 Acc: 0.6045\n",
      "\n",
      "Training complete in 9m 26s\n",
      "Ensemble Loss : 1.106613, Best val Loss: 1.116758\n",
      "4\n",
      "Cycle 0: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.4820 Acc: 0.4690\n",
      "val Loss: 1.4446 Acc: 0.4726\n",
      "\n",
      "Cycle 0: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.3873 Acc: 0.5181\n",
      "val Loss: 1.3619 Acc: 0.5172\n",
      "\n",
      "Cycle 0: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.3559 Acc: 0.5277\n",
      "val Loss: 1.3424 Acc: 0.5233\n",
      "\n",
      "Cycle 0: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.3195 Acc: 0.5381\n",
      "val Loss: 1.2968 Acc: 0.5294\n",
      "\n",
      "Cycle 0: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.2875 Acc: 0.5546\n",
      "val Loss: 1.2870 Acc: 0.5497\n",
      "\n",
      "Cycle 0: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.2573 Acc: 0.5614\n",
      "val Loss: 1.2777 Acc: 0.5477\n",
      "\n",
      "Cycle 0: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.2369 Acc: 0.5696\n",
      "val Loss: 1.2500 Acc: 0.5538\n",
      "\n",
      "Cycle 0: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.2154 Acc: 0.5704\n",
      "val Loss: 1.2514 Acc: 0.5416\n",
      "\n",
      "Cycle 0: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.1973 Acc: 0.5786\n",
      "val Loss: 1.2147 Acc: 0.5700\n",
      "\n",
      "Cycle 0: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.1826 Acc: 0.5811\n",
      "val Loss: 1.2119 Acc: 0.5680\n",
      "\n",
      "Cycle 1: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.2807 Acc: 0.5532\n",
      "val Loss: 1.2741 Acc: 0.5396\n",
      "\n",
      "Cycle 1: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.2501 Acc: 0.5661\n",
      "val Loss: 1.2201 Acc: 0.5700\n",
      "\n",
      "Cycle 1: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.2326 Acc: 0.5757\n",
      "val Loss: 1.2431 Acc: 0.5659\n",
      "\n",
      "Cycle 1: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.2158 Acc: 0.5704\n",
      "val Loss: 1.2189 Acc: 0.5700\n",
      "\n",
      "Cycle 1: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1928 Acc: 0.5868\n",
      "val Loss: 1.2178 Acc: 0.5598\n",
      "\n",
      "Cycle 1: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.1658 Acc: 0.5976\n",
      "val Loss: 1.2065 Acc: 0.5761\n",
      "\n",
      "Cycle 1: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.1442 Acc: 0.6069\n",
      "val Loss: 1.1630 Acc: 0.5984\n",
      "\n",
      "Cycle 1: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.1245 Acc: 0.6097\n",
      "val Loss: 1.1665 Acc: 0.5903\n",
      "\n",
      "Cycle 1: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0947 Acc: 0.6194\n",
      "val Loss: 1.1550 Acc: 0.5943\n",
      "\n",
      "Cycle 1: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0814 Acc: 0.6259\n",
      "val Loss: 1.1514 Acc: 0.6024\n",
      "\n",
      "Cycle 2: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.2225 Acc: 0.5693\n",
      "val Loss: 1.1972 Acc: 0.5862\n",
      "\n",
      "Cycle 2: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1887 Acc: 0.5832\n",
      "val Loss: 1.2120 Acc: 0.5720\n",
      "\n",
      "Cycle 2: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1604 Acc: 0.5897\n",
      "val Loss: 1.2552 Acc: 0.5538\n",
      "\n",
      "Cycle 2: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1503 Acc: 0.5904\n",
      "val Loss: 1.1936 Acc: 0.5882\n",
      "\n",
      "Cycle 2: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1249 Acc: 0.6037\n",
      "val Loss: 1.1729 Acc: 0.5761\n",
      "\n",
      "Cycle 2: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.0952 Acc: 0.6183\n",
      "val Loss: 1.2091 Acc: 0.5700\n",
      "\n",
      "Cycle 2: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0824 Acc: 0.6237\n",
      "val Loss: 1.1482 Acc: 0.5882\n",
      "\n",
      "Cycle 2: Epoch 7/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0543 Acc: 0.6323\n",
      "val Loss: 1.1488 Acc: 0.5984\n",
      "\n",
      "Cycle 2: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0413 Acc: 0.6301\n",
      "val Loss: 1.1430 Acc: 0.6024\n",
      "\n",
      "Cycle 2: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0314 Acc: 0.6319\n",
      "val Loss: 1.1417 Acc: 0.5963\n",
      "\n",
      "Cycle 3: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1592 Acc: 0.5854\n",
      "val Loss: 1.2328 Acc: 0.5578\n",
      "\n",
      "Cycle 3: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1343 Acc: 0.6076\n",
      "val Loss: 1.2156 Acc: 0.5740\n",
      "\n",
      "Cycle 3: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1232 Acc: 0.5961\n",
      "val Loss: 1.1911 Acc: 0.5923\n",
      "\n",
      "Cycle 3: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1037 Acc: 0.6126\n",
      "val Loss: 1.1892 Acc: 0.5882\n",
      "\n",
      "Cycle 3: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0784 Acc: 0.6208\n",
      "val Loss: 1.1555 Acc: 0.6004\n",
      "\n",
      "Cycle 3: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.0421 Acc: 0.6334\n",
      "val Loss: 1.1469 Acc: 0.5984\n",
      "\n",
      "Cycle 3: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0128 Acc: 0.6402\n",
      "val Loss: 1.1694 Acc: 0.5963\n",
      "\n",
      "Cycle 3: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9932 Acc: 0.6473\n",
      "val Loss: 1.1549 Acc: 0.6045\n",
      "\n",
      "Cycle 3: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9763 Acc: 0.6502\n",
      "val Loss: 1.1435 Acc: 0.6024\n",
      "\n",
      "Cycle 3: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.9583 Acc: 0.6638\n",
      "val Loss: 1.1474 Acc: 0.6024\n",
      "\n",
      "Cycle 4: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1109 Acc: 0.6108\n",
      "val Loss: 1.1867 Acc: 0.5882\n",
      "\n",
      "Cycle 4: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1020 Acc: 0.6051\n",
      "val Loss: 1.2025 Acc: 0.5720\n",
      "\n",
      "Cycle 4: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0827 Acc: 0.6126\n",
      "val Loss: 1.2309 Acc: 0.5639\n",
      "\n",
      "Cycle 4: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0595 Acc: 0.6241\n",
      "val Loss: 1.1676 Acc: 0.5801\n",
      "\n",
      "Cycle 4: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0108 Acc: 0.6366\n",
      "val Loss: 1.1969 Acc: 0.5882\n",
      "\n",
      "Cycle 4: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9908 Acc: 0.6434\n",
      "val Loss: 1.1895 Acc: 0.5781\n",
      "\n",
      "Cycle 4: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.9608 Acc: 0.6545\n",
      "val Loss: 1.1622 Acc: 0.5963\n",
      "\n",
      "Cycle 4: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9162 Acc: 0.6702\n",
      "val Loss: 1.1589 Acc: 0.6105\n",
      "\n",
      "Cycle 4: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9160 Acc: 0.6688\n",
      "val Loss: 1.1573 Acc: 0.6105\n",
      "\n",
      "Cycle 4: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.9067 Acc: 0.6738\n",
      "val Loss: 1.1597 Acc: 0.6146\n",
      "\n",
      "Cycle 5: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.0582 Acc: 0.6241\n",
      "val Loss: 1.2420 Acc: 0.5619\n",
      "\n",
      "Cycle 5: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0443 Acc: 0.6183\n",
      "val Loss: 1.1884 Acc: 0.5882\n",
      "\n",
      "Cycle 5: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0179 Acc: 0.6352\n",
      "val Loss: 1.1777 Acc: 0.5761\n",
      "\n",
      "Cycle 5: Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.9875 Acc: 0.6459\n",
      "val Loss: 1.2152 Acc: 0.5923\n",
      "\n",
      "Cycle 5: Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.9652 Acc: 0.6509\n",
      "val Loss: 1.2151 Acc: 0.5497\n",
      "\n",
      "Cycle 5: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9160 Acc: 0.6699\n",
      "val Loss: 1.2003 Acc: 0.6105\n",
      "\n",
      "Cycle 5: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.8950 Acc: 0.6663\n",
      "val Loss: 1.1814 Acc: 0.6045\n",
      "\n",
      "Cycle 5: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.8404 Acc: 0.6950\n",
      "val Loss: 1.2023 Acc: 0.5943\n",
      "\n",
      "Cycle 5: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.8287 Acc: 0.7000\n",
      "val Loss: 1.1977 Acc: 0.6004\n",
      "\n",
      "Cycle 5: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.8148 Acc: 0.7103\n",
      "val Loss: 1.1977 Acc: 0.6065\n",
      "\n",
      "Training complete in 9m 31s\n",
      "Ensemble Loss : 1.129699, Best val Loss: 1.141747\n",
      "5\n",
      "Cycle 0: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.4716 Acc: 0.4701\n",
      "val Loss: 1.4312 Acc: 0.4909\n",
      "\n",
      "Cycle 0: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.3840 Acc: 0.5149\n",
      "val Loss: 1.3565 Acc: 0.5091\n",
      "\n",
      "Cycle 0: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.3476 Acc: 0.5299\n",
      "val Loss: 1.3146 Acc: 0.5396\n",
      "\n",
      "Cycle 0: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.3265 Acc: 0.5342\n",
      "val Loss: 1.3207 Acc: 0.5396\n",
      "\n",
      "Cycle 0: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.2851 Acc: 0.5485\n",
      "val Loss: 1.2776 Acc: 0.5538\n",
      "\n",
      "Cycle 0: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.2519 Acc: 0.5661\n",
      "val Loss: 1.2666 Acc: 0.5700\n",
      "\n",
      "Cycle 0: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.2356 Acc: 0.5686\n",
      "val Loss: 1.2634 Acc: 0.5517\n",
      "\n",
      "Cycle 0: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.2059 Acc: 0.5772\n",
      "val Loss: 1.2229 Acc: 0.5842\n",
      "\n",
      "Cycle 0: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.1914 Acc: 0.5850\n",
      "val Loss: 1.2173 Acc: 0.5822\n",
      "\n",
      "Cycle 0: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.1644 Acc: 0.5915\n",
      "val Loss: 1.2117 Acc: 0.5842\n",
      "\n",
      "Cycle 1: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.2832 Acc: 0.5474\n",
      "val Loss: 1.2855 Acc: 0.5456\n",
      "\n",
      "Cycle 1: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.2460 Acc: 0.5639\n",
      "val Loss: 1.2603 Acc: 0.5659\n",
      "\n",
      "Cycle 1: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.2351 Acc: 0.5696\n",
      "val Loss: 1.2431 Acc: 0.5801\n",
      "\n",
      "Cycle 1: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.2110 Acc: 0.5661\n",
      "val Loss: 1.2228 Acc: 0.5842\n",
      "\n",
      "Cycle 1: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1924 Acc: 0.5729\n",
      "val Loss: 1.2349 Acc: 0.5943\n",
      "\n",
      "Cycle 1: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.1720 Acc: 0.5825\n",
      "val Loss: 1.1860 Acc: 0.6004\n",
      "\n",
      "Cycle 1: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.1371 Acc: 0.5954\n",
      "val Loss: 1.1791 Acc: 0.6187\n",
      "\n",
      "Cycle 1: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.1172 Acc: 0.6040\n",
      "val Loss: 1.1629 Acc: 0.6146\n",
      "\n",
      "Cycle 1: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0947 Acc: 0.6094\n",
      "val Loss: 1.1469 Acc: 0.6207\n",
      "\n",
      "Cycle 1: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0826 Acc: 0.6176\n",
      "val Loss: 1.1498 Acc: 0.6268\n",
      "\n",
      "Cycle 2: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.2011 Acc: 0.5807\n",
      "val Loss: 1.2453 Acc: 0.5761\n",
      "\n",
      "Cycle 2: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1865 Acc: 0.5818\n",
      "val Loss: 1.2083 Acc: 0.5842\n",
      "\n",
      "Cycle 2: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1624 Acc: 0.5850\n",
      "val Loss: 1.1688 Acc: 0.5943\n",
      "\n",
      "Cycle 2: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1488 Acc: 0.5951\n",
      "val Loss: 1.2056 Acc: 0.6045\n",
      "\n",
      "Cycle 2: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1263 Acc: 0.6072\n",
      "val Loss: 1.1669 Acc: 0.6105\n",
      "\n",
      "Cycle 2: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.0933 Acc: 0.6101\n",
      "val Loss: 1.1662 Acc: 0.6085\n",
      "\n",
      "Cycle 2: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0798 Acc: 0.6162\n",
      "val Loss: 1.1416 Acc: 0.6308\n",
      "\n",
      "Cycle 2: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.0495 Acc: 0.6259\n",
      "val Loss: 1.1370 Acc: 0.6166\n",
      "\n",
      "Cycle 2: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0434 Acc: 0.6294\n",
      "val Loss: 1.1348 Acc: 0.6227\n",
      "\n",
      "Cycle 2: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0251 Acc: 0.6405\n",
      "val Loss: 1.1341 Acc: 0.6207\n",
      "\n",
      "Cycle 3: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1581 Acc: 0.5926\n",
      "val Loss: 1.1795 Acc: 0.6166\n",
      "\n",
      "Cycle 3: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1421 Acc: 0.5951\n",
      "val Loss: 1.2216 Acc: 0.5781\n",
      "\n",
      "Cycle 3: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1141 Acc: 0.6026\n",
      "val Loss: 1.1595 Acc: 0.6105\n",
      "\n",
      "Cycle 3: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0862 Acc: 0.6176\n",
      "val Loss: 1.1906 Acc: 0.5822\n",
      "\n",
      "Cycle 3: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0808 Acc: 0.6173\n",
      "val Loss: 1.1803 Acc: 0.5923\n",
      "\n",
      "Cycle 3: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.0486 Acc: 0.6266\n",
      "val Loss: 1.1670 Acc: 0.6024\n",
      "\n",
      "Cycle 3: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0128 Acc: 0.6337\n",
      "val Loss: 1.1395 Acc: 0.6227\n",
      "\n",
      "Cycle 3: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.0041 Acc: 0.6491\n",
      "val Loss: 1.1309 Acc: 0.6187\n",
      "\n",
      "Cycle 3: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9645 Acc: 0.6534\n",
      "val Loss: 1.1378 Acc: 0.6207\n",
      "\n",
      "Cycle 3: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.9693 Acc: 0.6624\n",
      "val Loss: 1.1360 Acc: 0.6207\n",
      "\n",
      "Cycle 4: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1121 Acc: 0.6029\n",
      "val Loss: 1.2207 Acc: 0.5720\n",
      "\n",
      "Cycle 4: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0864 Acc: 0.6230\n",
      "val Loss: 1.2107 Acc: 0.5842\n",
      "\n",
      "Cycle 4: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0566 Acc: 0.6276\n",
      "val Loss: 1.1608 Acc: 0.5903\n",
      "\n",
      "Cycle 4: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0465 Acc: 0.6298\n",
      "val Loss: 1.1873 Acc: 0.5801\n",
      "\n",
      "Cycle 4: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0243 Acc: 0.6362\n",
      "val Loss: 1.2019 Acc: 0.5923\n",
      "\n",
      "Cycle 4: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9947 Acc: 0.6445\n",
      "val Loss: 1.1775 Acc: 0.5923\n",
      "\n",
      "Cycle 4: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.9643 Acc: 0.6484\n",
      "val Loss: 1.1428 Acc: 0.6166\n",
      "\n",
      "Cycle 4: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9363 Acc: 0.6559\n",
      "val Loss: 1.1526 Acc: 0.6004\n",
      "\n",
      "Cycle 4: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9105 Acc: 0.6720\n",
      "val Loss: 1.1457 Acc: 0.6085\n",
      "\n",
      "Cycle 4: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.8943 Acc: 0.6828\n",
      "val Loss: 1.1469 Acc: 0.6065\n",
      "\n",
      "Cycle 5: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.0573 Acc: 0.6183\n",
      "val Loss: 1.2408 Acc: 0.6126\n",
      "\n",
      "Cycle 5: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0488 Acc: 0.6183\n",
      "val Loss: 1.2311 Acc: 0.5822\n",
      "\n",
      "Cycle 5: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0129 Acc: 0.6312\n",
      "val Loss: 1.2665 Acc: 0.5659\n",
      "\n",
      "Cycle 5: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0040 Acc: 0.6355\n",
      "val Loss: 1.1898 Acc: 0.5882\n",
      "\n",
      "Cycle 5: Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.9529 Acc: 0.6570\n",
      "val Loss: 1.2206 Acc: 0.5680\n",
      "\n",
      "Cycle 5: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9394 Acc: 0.6574\n",
      "val Loss: 1.1475 Acc: 0.6085\n",
      "\n",
      "Cycle 5: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.8982 Acc: 0.6699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.1434 Acc: 0.6085\n",
      "\n",
      "Cycle 5: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.8632 Acc: 0.6835\n",
      "val Loss: 1.1616 Acc: 0.6004\n",
      "\n",
      "Cycle 5: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.8306 Acc: 0.6942\n",
      "val Loss: 1.1584 Acc: 0.6045\n",
      "\n",
      "Cycle 5: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.8171 Acc: 0.6985\n",
      "val Loss: 1.1616 Acc: 0.6085\n",
      "\n",
      "Training complete in 9m 24s\n",
      "Ensemble Loss : 1.121903, Best val Loss: 1.130940\n",
      "6\n",
      "Cycle 0: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.4740 Acc: 0.4658\n",
      "val Loss: 1.3861 Acc: 0.5132\n",
      "\n",
      "Cycle 0: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.3899 Acc: 0.5141\n",
      "val Loss: 1.3684 Acc: 0.5193\n",
      "\n",
      "Cycle 0: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.3408 Acc: 0.5292\n",
      "val Loss: 1.3173 Acc: 0.5497\n",
      "\n",
      "Cycle 0: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.3060 Acc: 0.5410\n",
      "val Loss: 1.2781 Acc: 0.5416\n",
      "\n",
      "Cycle 0: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.2696 Acc: 0.5607\n",
      "val Loss: 1.3315 Acc: 0.5193\n",
      "\n",
      "Cycle 0: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.2541 Acc: 0.5639\n",
      "val Loss: 1.2596 Acc: 0.5497\n",
      "\n",
      "Cycle 0: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.2113 Acc: 0.5811\n",
      "val Loss: 1.2246 Acc: 0.5598\n",
      "\n",
      "Cycle 0: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.1948 Acc: 0.5829\n",
      "val Loss: 1.2061 Acc: 0.5680\n",
      "\n",
      "Cycle 0: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.1727 Acc: 0.5908\n",
      "val Loss: 1.1972 Acc: 0.5639\n",
      "\n",
      "Cycle 0: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.1628 Acc: 0.5958\n",
      "val Loss: 1.1927 Acc: 0.5680\n",
      "\n",
      "Cycle 1: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.2655 Acc: 0.5636\n",
      "val Loss: 1.2661 Acc: 0.5598\n",
      "\n",
      "Cycle 1: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.2467 Acc: 0.5682\n",
      "val Loss: 1.3366 Acc: 0.5091\n",
      "\n",
      "Cycle 1: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.2282 Acc: 0.5646\n",
      "val Loss: 1.2203 Acc: 0.5740\n",
      "\n",
      "Cycle 1: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1918 Acc: 0.5865\n",
      "val Loss: 1.2667 Acc: 0.5639\n",
      "\n",
      "Cycle 1: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1793 Acc: 0.5897\n",
      "val Loss: 1.1959 Acc: 0.5659\n",
      "\n",
      "Cycle 1: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.1592 Acc: 0.5897\n",
      "val Loss: 1.1834 Acc: 0.5558\n",
      "\n",
      "Cycle 1: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.1415 Acc: 0.6083\n",
      "val Loss: 1.1617 Acc: 0.5761\n",
      "\n",
      "Cycle 1: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.1078 Acc: 0.6119\n",
      "val Loss: 1.1384 Acc: 0.5822\n",
      "\n",
      "Cycle 1: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0938 Acc: 0.6180\n",
      "val Loss: 1.1378 Acc: 0.5842\n",
      "\n",
      "Cycle 1: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0848 Acc: 0.6198\n",
      "val Loss: 1.1381 Acc: 0.5842\n",
      "\n",
      "Cycle 2: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1976 Acc: 0.5775\n",
      "val Loss: 1.2363 Acc: 0.5558\n",
      "\n",
      "Cycle 2: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1914 Acc: 0.5800\n",
      "val Loss: 1.2171 Acc: 0.5497\n",
      "\n",
      "Cycle 2: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1626 Acc: 0.6015\n",
      "val Loss: 1.1911 Acc: 0.5822\n",
      "\n",
      "Cycle 2: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1437 Acc: 0.5997\n",
      "val Loss: 1.1584 Acc: 0.5781\n",
      "\n",
      "Cycle 2: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1323 Acc: 0.6054\n",
      "val Loss: 1.1398 Acc: 0.5801\n",
      "\n",
      "Cycle 2: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.1075 Acc: 0.6083\n",
      "val Loss: 1.1523 Acc: 0.5822\n",
      "\n",
      "Cycle 2: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0723 Acc: 0.6266\n",
      "val Loss: 1.1372 Acc: 0.5842\n",
      "\n",
      "Cycle 2: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.0404 Acc: 0.6291\n",
      "val Loss: 1.1260 Acc: 0.5862\n",
      "\n",
      "Cycle 2: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0241 Acc: 0.6441\n",
      "val Loss: 1.1211 Acc: 0.5943\n",
      "\n",
      "Cycle 2: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0237 Acc: 0.6448\n",
      "val Loss: 1.1213 Acc: 0.5984\n",
      "\n",
      "Cycle 3: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1505 Acc: 0.5976\n",
      "val Loss: 1.2442 Acc: 0.5477\n",
      "\n",
      "Cycle 3: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1438 Acc: 0.5972\n",
      "val Loss: 1.1795 Acc: 0.5781\n",
      "\n",
      "Cycle 3: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1012 Acc: 0.6137\n",
      "val Loss: 1.1766 Acc: 0.5923\n",
      "\n",
      "Cycle 3: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0917 Acc: 0.6126\n",
      "val Loss: 1.1552 Acc: 0.5963\n",
      "\n",
      "Cycle 3: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0789 Acc: 0.6287\n",
      "val Loss: 1.1361 Acc: 0.5720\n",
      "\n",
      "Cycle 3: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.0420 Acc: 0.6319\n",
      "val Loss: 1.1244 Acc: 0.5903\n",
      "\n",
      "Cycle 3: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0055 Acc: 0.6477\n",
      "val Loss: 1.1627 Acc: 0.5923\n",
      "\n",
      "Cycle 3: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9820 Acc: 0.6455\n",
      "val Loss: 1.1313 Acc: 0.6024\n",
      "\n",
      "Cycle 3: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9651 Acc: 0.6506\n",
      "val Loss: 1.1203 Acc: 0.5882\n",
      "\n",
      "Cycle 3: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.9811 Acc: 0.6538\n",
      "val Loss: 1.1183 Acc: 0.5943\n",
      "\n",
      "Cycle 4: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1020 Acc: 0.6155\n",
      "val Loss: 1.1824 Acc: 0.5882\n",
      "\n",
      "Cycle 4: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0962 Acc: 0.6101\n",
      "val Loss: 1.1695 Acc: 0.5923\n",
      "\n",
      "Cycle 4: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0586 Acc: 0.6266\n",
      "val Loss: 1.2027 Acc: 0.5740\n",
      "\n",
      "Cycle 4: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0537 Acc: 0.6334\n",
      "val Loss: 1.1744 Acc: 0.5801\n",
      "\n",
      "Cycle 4: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0169 Acc: 0.6327\n",
      "val Loss: 1.1434 Acc: 0.5801\n",
      "\n",
      "Cycle 4: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.0025 Acc: 0.6369\n",
      "val Loss: 1.1197 Acc: 0.5963\n",
      "\n",
      "Cycle 4: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.9629 Acc: 0.6556\n",
      "val Loss: 1.1177 Acc: 0.5984\n",
      "\n",
      "Cycle 4: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9320 Acc: 0.6645\n",
      "val Loss: 1.1107 Acc: 0.5943\n",
      "\n",
      "Cycle 4: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9112 Acc: 0.6742\n",
      "val Loss: 1.1083 Acc: 0.6004\n",
      "\n",
      "Cycle 4: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.8988 Acc: 0.6767\n",
      "val Loss: 1.1082 Acc: 0.6004\n",
      "\n",
      "Cycle 5: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.0563 Acc: 0.6201\n",
      "val Loss: 1.2143 Acc: 0.5882\n",
      "\n",
      "Cycle 5: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0455 Acc: 0.6319\n",
      "val Loss: 1.2081 Acc: 0.5700\n",
      "\n",
      "Cycle 5: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0231 Acc: 0.6344\n",
      "val Loss: 1.1494 Acc: 0.5903\n",
      "\n",
      "Cycle 5: Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.9867 Acc: 0.6520\n",
      "val Loss: 1.1439 Acc: 0.5943\n",
      "\n",
      "Cycle 5: Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.9577 Acc: 0.6577\n",
      "val Loss: 1.1540 Acc: 0.5659\n",
      "\n",
      "Cycle 5: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9208 Acc: 0.6756\n",
      "val Loss: 1.1677 Acc: 0.5477\n",
      "\n",
      "Cycle 5: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.8980 Acc: 0.6803\n",
      "val Loss: 1.1423 Acc: 0.5700\n",
      "\n",
      "Cycle 5: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.8606 Acc: 0.6971\n",
      "val Loss: 1.1333 Acc: 0.5761\n",
      "\n",
      "Cycle 5: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.8268 Acc: 0.6975\n",
      "val Loss: 1.1235 Acc: 0.5801\n",
      "\n",
      "Cycle 5: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.8099 Acc: 0.7025\n",
      "val Loss: 1.1224 Acc: 0.5781\n",
      "\n",
      "Training complete in 9m 35s\n",
      "Ensemble Loss : 1.103994, Best val Loss: 1.108211\n",
      "7\n",
      "Cycle 0: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.4783 Acc: 0.4733\n",
      "val Loss: 1.3831 Acc: 0.5314\n",
      "\n",
      "Cycle 0: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.3888 Acc: 0.5131\n",
      "val Loss: 1.3485 Acc: 0.5254\n",
      "\n",
      "Cycle 0: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.3511 Acc: 0.5260\n",
      "val Loss: 1.3278 Acc: 0.5254\n",
      "\n",
      "Cycle 0: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.3190 Acc: 0.5388\n",
      "val Loss: 1.3558 Acc: 0.5193\n",
      "\n",
      "Cycle 0: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.2868 Acc: 0.5410\n",
      "val Loss: 1.2841 Acc: 0.5436\n",
      "\n",
      "Cycle 0: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.2692 Acc: 0.5514\n",
      "val Loss: 1.2861 Acc: 0.5517\n",
      "\n",
      "Cycle 0: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.2360 Acc: 0.5621\n",
      "val Loss: 1.2523 Acc: 0.5761\n",
      "\n",
      "Cycle 0: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.2191 Acc: 0.5700\n",
      "val Loss: 1.2184 Acc: 0.5781\n",
      "\n",
      "Cycle 0: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.2025 Acc: 0.5711\n",
      "val Loss: 1.2104 Acc: 0.5943\n",
      "\n",
      "Cycle 0: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.1870 Acc: 0.5815\n",
      "val Loss: 1.2113 Acc: 0.5842\n",
      "\n",
      "Cycle 1: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.2797 Acc: 0.5528\n",
      "val Loss: 1.2520 Acc: 0.5761\n",
      "\n",
      "Cycle 1: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.2516 Acc: 0.5567\n",
      "val Loss: 1.2185 Acc: 0.5882\n",
      "\n",
      "Cycle 1: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.2193 Acc: 0.5743\n",
      "val Loss: 1.3151 Acc: 0.5375\n",
      "\n",
      "Cycle 1: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.2133 Acc: 0.5825\n",
      "val Loss: 1.2011 Acc: 0.5923\n",
      "\n",
      "Cycle 1: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1858 Acc: 0.5822\n",
      "val Loss: 1.2014 Acc: 0.6045\n",
      "\n",
      "Cycle 1: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.1722 Acc: 0.5825\n",
      "val Loss: 1.1830 Acc: 0.5882\n",
      "\n",
      "Cycle 1: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.1393 Acc: 0.6033\n",
      "val Loss: 1.1783 Acc: 0.5984\n",
      "\n",
      "Cycle 1: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.1076 Acc: 0.6062\n",
      "val Loss: 1.1755 Acc: 0.5943\n",
      "\n",
      "Cycle 1: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.1018 Acc: 0.6076\n",
      "val Loss: 1.1664 Acc: 0.6045\n",
      "\n",
      "Cycle 1: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0891 Acc: 0.6241\n",
      "val Loss: 1.1631 Acc: 0.5943\n",
      "\n",
      "Cycle 2: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.2006 Acc: 0.5668\n",
      "val Loss: 1.1813 Acc: 0.6065\n",
      "\n",
      "Cycle 2: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1786 Acc: 0.5843\n",
      "val Loss: 1.1863 Acc: 0.6024\n",
      "\n",
      "Cycle 2: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1579 Acc: 0.5865\n",
      "val Loss: 1.2401 Acc: 0.5862\n",
      "\n",
      "Cycle 2: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1579 Acc: 0.5886\n",
      "val Loss: 1.2157 Acc: 0.5882\n",
      "\n",
      "Cycle 2: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1231 Acc: 0.5990\n",
      "val Loss: 1.1866 Acc: 0.5963\n",
      "\n",
      "Cycle 2: Epoch 5/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0922 Acc: 0.6079\n",
      "val Loss: 1.1883 Acc: 0.5943\n",
      "\n",
      "Cycle 2: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0662 Acc: 0.6194\n",
      "val Loss: 1.1455 Acc: 0.6045\n",
      "\n",
      "Cycle 2: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.0595 Acc: 0.6173\n",
      "val Loss: 1.1580 Acc: 0.6004\n",
      "\n",
      "Cycle 2: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0287 Acc: 0.6395\n",
      "val Loss: 1.1475 Acc: 0.6065\n",
      "\n",
      "Cycle 2: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0191 Acc: 0.6412\n",
      "val Loss: 1.1483 Acc: 0.6065\n",
      "\n",
      "Cycle 3: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1537 Acc: 0.5865\n",
      "val Loss: 1.3579 Acc: 0.5375\n",
      "\n",
      "Cycle 3: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1421 Acc: 0.5983\n",
      "val Loss: 1.2624 Acc: 0.5822\n",
      "\n",
      "Cycle 3: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1141 Acc: 0.6058\n",
      "val Loss: 1.2067 Acc: 0.5984\n",
      "\n",
      "Cycle 3: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1033 Acc: 0.6169\n",
      "val Loss: 1.1538 Acc: 0.6085\n",
      "\n",
      "Cycle 3: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0534 Acc: 0.6280\n",
      "val Loss: 1.1805 Acc: 0.6045\n",
      "\n",
      "Cycle 3: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.0435 Acc: 0.6373\n",
      "val Loss: 1.1704 Acc: 0.6146\n",
      "\n",
      "Cycle 3: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0266 Acc: 0.6284\n",
      "val Loss: 1.1412 Acc: 0.6126\n",
      "\n",
      "Cycle 3: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9961 Acc: 0.6391\n",
      "val Loss: 1.1610 Acc: 0.5943\n",
      "\n",
      "Cycle 3: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9544 Acc: 0.6559\n",
      "val Loss: 1.1533 Acc: 0.6024\n",
      "\n",
      "Cycle 3: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.9585 Acc: 0.6559\n",
      "val Loss: 1.1508 Acc: 0.6004\n",
      "\n",
      "Cycle 4: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1061 Acc: 0.6040\n",
      "val Loss: 1.1963 Acc: 0.5862\n",
      "\n",
      "Cycle 4: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0794 Acc: 0.6158\n",
      "val Loss: 1.2770 Acc: 0.5680\n",
      "\n",
      "Cycle 4: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0692 Acc: 0.6205\n",
      "val Loss: 1.1933 Acc: 0.5842\n",
      "\n",
      "Cycle 4: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0527 Acc: 0.6298\n",
      "val Loss: 1.1890 Acc: 0.6085\n",
      "\n",
      "Cycle 4: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0146 Acc: 0.6341\n",
      "val Loss: 1.1749 Acc: 0.5903\n",
      "\n",
      "Cycle 4: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9744 Acc: 0.6527\n",
      "val Loss: 1.1945 Acc: 0.5761\n",
      "\n",
      "Cycle 4: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.9442 Acc: 0.6617\n",
      "val Loss: 1.2258 Acc: 0.5781\n",
      "\n",
      "Cycle 4: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9277 Acc: 0.6660\n",
      "val Loss: 1.1796 Acc: 0.6004\n",
      "\n",
      "Cycle 4: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9126 Acc: 0.6713\n",
      "val Loss: 1.1699 Acc: 0.6045\n",
      "\n",
      "Cycle 4: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.8961 Acc: 0.6756\n",
      "val Loss: 1.1664 Acc: 0.6024\n",
      "\n",
      "Cycle 5: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.0451 Acc: 0.6251\n",
      "val Loss: 1.2071 Acc: 0.5984\n",
      "\n",
      "Cycle 5: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0429 Acc: 0.6151\n",
      "val Loss: 1.2061 Acc: 0.5781\n",
      "\n",
      "Cycle 5: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0057 Acc: 0.6312\n",
      "val Loss: 1.2379 Acc: 0.5903\n",
      "\n",
      "Cycle 5: Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.9858 Acc: 0.6398\n",
      "val Loss: 1.2107 Acc: 0.5984\n",
      "\n",
      "Cycle 5: Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.9495 Acc: 0.6566\n",
      "val Loss: 1.2183 Acc: 0.5943\n",
      "\n",
      "Cycle 5: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.8933 Acc: 0.6785\n",
      "val Loss: 1.2482 Acc: 0.6004\n",
      "\n",
      "Cycle 5: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.8782 Acc: 0.6828\n",
      "val Loss: 1.2245 Acc: 0.5842\n",
      "\n",
      "Cycle 5: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.8204 Acc: 0.7000\n",
      "val Loss: 1.2396 Acc: 0.6045\n",
      "\n",
      "Cycle 5: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.8100 Acc: 0.7118\n",
      "val Loss: 1.2268 Acc: 0.5984\n",
      "\n",
      "Cycle 5: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.8080 Acc: 0.7161\n",
      "val Loss: 1.2248 Acc: 0.5984\n",
      "\n",
      "Training complete in 9m 58s\n",
      "Ensemble Loss : 1.141074, Best val Loss: 1.141157\n",
      "8\n",
      "Cycle 0: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.4723 Acc: 0.4730\n",
      "val Loss: 1.4146 Acc: 0.4848\n",
      "\n",
      "Cycle 0: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.3824 Acc: 0.5141\n",
      "val Loss: 1.3623 Acc: 0.5254\n",
      "\n",
      "Cycle 0: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.3507 Acc: 0.5292\n",
      "val Loss: 1.3679 Acc: 0.5233\n",
      "\n",
      "Cycle 0: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.3189 Acc: 0.5421\n",
      "val Loss: 1.3018 Acc: 0.5659\n",
      "\n",
      "Cycle 0: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.2905 Acc: 0.5424\n",
      "val Loss: 1.2937 Acc: 0.5538\n",
      "\n",
      "Cycle 0: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.2689 Acc: 0.5485\n",
      "val Loss: 1.2602 Acc: 0.5598\n",
      "\n",
      "Cycle 0: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.2369 Acc: 0.5693\n",
      "val Loss: 1.2300 Acc: 0.5740\n",
      "\n",
      "Cycle 0: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.2168 Acc: 0.5661\n",
      "val Loss: 1.2139 Acc: 0.5882\n",
      "\n",
      "Cycle 0: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.1862 Acc: 0.5861\n",
      "val Loss: 1.1966 Acc: 0.5903\n",
      "\n",
      "Cycle 0: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.1928 Acc: 0.5832\n",
      "val Loss: 1.1935 Acc: 0.5862\n",
      "\n",
      "Cycle 1: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.2715 Acc: 0.5682\n",
      "val Loss: 1.2577 Acc: 0.5822\n",
      "\n",
      "Cycle 1: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.2631 Acc: 0.5664\n",
      "val Loss: 1.2619 Acc: 0.5720\n",
      "\n",
      "Cycle 1: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.2294 Acc: 0.5707\n",
      "val Loss: 1.2528 Acc: 0.5882\n",
      "\n",
      "Cycle 1: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1962 Acc: 0.5829\n",
      "val Loss: 1.2107 Acc: 0.5984\n",
      "\n",
      "Cycle 1: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1855 Acc: 0.5865\n",
      "val Loss: 1.1900 Acc: 0.6207\n",
      "\n",
      "Cycle 1: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.1582 Acc: 0.5847\n",
      "val Loss: 1.1978 Acc: 0.6166\n",
      "\n",
      "Cycle 1: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.1356 Acc: 0.6004\n",
      "val Loss: 1.1669 Acc: 0.5963\n",
      "\n",
      "Cycle 1: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.1133 Acc: 0.6037\n",
      "val Loss: 1.1663 Acc: 0.6024\n",
      "\n",
      "Cycle 1: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.1017 Acc: 0.6072\n",
      "val Loss: 1.1435 Acc: 0.6105\n",
      "\n",
      "Cycle 1: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0810 Acc: 0.6190\n",
      "val Loss: 1.1431 Acc: 0.6085\n",
      "\n",
      "Cycle 2: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1981 Acc: 0.5757\n",
      "val Loss: 1.2503 Acc: 0.5781\n",
      "\n",
      "Cycle 2: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.2048 Acc: 0.5782\n",
      "val Loss: 1.2065 Acc: 0.6004\n",
      "\n",
      "Cycle 2: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1716 Acc: 0.5947\n",
      "val Loss: 1.2033 Acc: 0.6024\n",
      "\n",
      "Cycle 2: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1516 Acc: 0.5983\n",
      "val Loss: 1.1644 Acc: 0.5842\n",
      "\n",
      "Cycle 2: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1434 Acc: 0.6072\n",
      "val Loss: 1.1683 Acc: 0.6166\n",
      "\n",
      "Cycle 2: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.1049 Acc: 0.6069\n",
      "val Loss: 1.1419 Acc: 0.6085\n",
      "\n",
      "Cycle 2: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0675 Acc: 0.6280\n",
      "val Loss: 1.1316 Acc: 0.6146\n",
      "\n",
      "Cycle 2: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.0595 Acc: 0.6276\n",
      "val Loss: 1.1362 Acc: 0.6146\n",
      "\n",
      "Cycle 2: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0420 Acc: 0.6298\n",
      "val Loss: 1.1272 Acc: 0.6227\n",
      "\n",
      "Cycle 2: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0224 Acc: 0.6420\n",
      "val Loss: 1.1300 Acc: 0.6187\n",
      "\n",
      "Cycle 3: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1662 Acc: 0.5854\n",
      "val Loss: 1.2305 Acc: 0.5740\n",
      "\n",
      "Cycle 3: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1438 Acc: 0.5911\n",
      "val Loss: 1.2021 Acc: 0.5903\n",
      "\n",
      "Cycle 3: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1182 Acc: 0.6062\n",
      "val Loss: 1.1511 Acc: 0.5903\n",
      "\n",
      "Cycle 3: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0937 Acc: 0.6205\n",
      "val Loss: 1.2707 Acc: 0.5700\n",
      "\n",
      "Cycle 3: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0816 Acc: 0.6155\n",
      "val Loss: 1.1772 Acc: 0.6065\n",
      "\n",
      "Cycle 3: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.0413 Acc: 0.6409\n",
      "val Loss: 1.1393 Acc: 0.5984\n",
      "\n",
      "Cycle 3: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0198 Acc: 0.6380\n",
      "val Loss: 1.1368 Acc: 0.6105\n",
      "\n",
      "Cycle 3: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9963 Acc: 0.6448\n",
      "val Loss: 1.1370 Acc: 0.6024\n",
      "\n",
      "Cycle 3: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9734 Acc: 0.6620\n",
      "val Loss: 1.1377 Acc: 0.6004\n",
      "\n",
      "Cycle 3: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.9560 Acc: 0.6627\n",
      "val Loss: 1.1375 Acc: 0.6085\n",
      "\n",
      "Cycle 4: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.0943 Acc: 0.6208\n",
      "val Loss: 1.2108 Acc: 0.5963\n",
      "\n",
      "Cycle 4: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1056 Acc: 0.5990\n",
      "val Loss: 1.1841 Acc: 0.5943\n",
      "\n",
      "Cycle 4: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0754 Acc: 0.6144\n",
      "val Loss: 1.1988 Acc: 0.5822\n",
      "\n",
      "Cycle 4: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0586 Acc: 0.6201\n",
      "val Loss: 1.1580 Acc: 0.6187\n",
      "\n",
      "Cycle 4: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0317 Acc: 0.6287\n",
      "val Loss: 1.1761 Acc: 0.6045\n",
      "\n",
      "Cycle 4: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9887 Acc: 0.6441\n",
      "val Loss: 1.1620 Acc: 0.6187\n",
      "\n",
      "Cycle 4: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.9632 Acc: 0.6617\n",
      "val Loss: 1.1631 Acc: 0.6126\n",
      "\n",
      "Cycle 4: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9390 Acc: 0.6663\n",
      "val Loss: 1.1420 Acc: 0.6207\n",
      "\n",
      "Cycle 4: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9200 Acc: 0.6652\n",
      "val Loss: 1.1485 Acc: 0.6207\n",
      "\n",
      "Cycle 4: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.9020 Acc: 0.6774\n",
      "val Loss: 1.1482 Acc: 0.6227\n",
      "\n",
      "Cycle 5: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.0642 Acc: 0.6212\n",
      "val Loss: 1.1876 Acc: 0.6045\n",
      "\n",
      "Cycle 5: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0483 Acc: 0.6255\n",
      "val Loss: 1.1924 Acc: 0.5963\n",
      "\n",
      "Cycle 5: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0333 Acc: 0.6212\n",
      "val Loss: 1.1902 Acc: 0.6166\n",
      "\n",
      "Cycle 5: Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.9826 Acc: 0.6384\n",
      "val Loss: 1.1761 Acc: 0.5963\n",
      "\n",
      "Cycle 5: Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.9569 Acc: 0.6595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 1.2069 Acc: 0.6085\n",
      "\n",
      "Cycle 5: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9272 Acc: 0.6645\n",
      "val Loss: 1.1988 Acc: 0.5822\n",
      "\n",
      "Cycle 5: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.8947 Acc: 0.6774\n",
      "val Loss: 1.2012 Acc: 0.6024\n",
      "\n",
      "Cycle 5: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.8549 Acc: 0.6975\n",
      "val Loss: 1.1957 Acc: 0.6105\n",
      "\n",
      "Cycle 5: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.8329 Acc: 0.7082\n",
      "val Loss: 1.1840 Acc: 0.6085\n",
      "\n",
      "Cycle 5: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.8064 Acc: 0.7204\n",
      "val Loss: 1.1857 Acc: 0.6126\n",
      "\n",
      "Training complete in 9m 42s\n",
      "Ensemble Loss : 1.118529, Best val Loss: 1.127232\n",
      "9\n",
      "Cycle 0: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.4778 Acc: 0.4719\n",
      "val Loss: 1.3919 Acc: 0.5172\n",
      "\n",
      "Cycle 0: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.3727 Acc: 0.5213\n",
      "val Loss: 1.3135 Acc: 0.5477\n",
      "\n",
      "Cycle 0: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.3586 Acc: 0.5209\n",
      "val Loss: 1.3237 Acc: 0.5436\n",
      "\n",
      "Cycle 0: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.3270 Acc: 0.5403\n",
      "val Loss: 1.3346 Acc: 0.5172\n",
      "\n",
      "Cycle 0: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.2962 Acc: 0.5456\n",
      "val Loss: 1.2841 Acc: 0.5598\n",
      "\n",
      "Cycle 0: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.2635 Acc: 0.5528\n",
      "val Loss: 1.2513 Acc: 0.5700\n",
      "\n",
      "Cycle 0: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.2469 Acc: 0.5607\n",
      "val Loss: 1.2254 Acc: 0.5680\n",
      "\n",
      "Cycle 0: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.2093 Acc: 0.5757\n",
      "val Loss: 1.2201 Acc: 0.5740\n",
      "\n",
      "Cycle 0: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.1962 Acc: 0.5800\n",
      "val Loss: 1.2142 Acc: 0.5761\n",
      "\n",
      "Cycle 0: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.1851 Acc: 0.5836\n",
      "val Loss: 1.2149 Acc: 0.5781\n",
      "\n",
      "Cycle 1: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.2824 Acc: 0.5636\n",
      "val Loss: 1.3338 Acc: 0.5254\n",
      "\n",
      "Cycle 1: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.2583 Acc: 0.5585\n",
      "val Loss: 1.2786 Acc: 0.5720\n",
      "\n",
      "Cycle 1: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.2236 Acc: 0.5761\n",
      "val Loss: 1.2373 Acc: 0.5700\n",
      "\n",
      "Cycle 1: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.2126 Acc: 0.5675\n",
      "val Loss: 1.2100 Acc: 0.5943\n",
      "\n",
      "Cycle 1: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1895 Acc: 0.5883\n",
      "val Loss: 1.1902 Acc: 0.5822\n",
      "\n",
      "Cycle 1: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.1451 Acc: 0.5926\n",
      "val Loss: 1.1941 Acc: 0.5943\n",
      "\n",
      "Cycle 1: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.1480 Acc: 0.5929\n",
      "val Loss: 1.1890 Acc: 0.5882\n",
      "\n",
      "Cycle 1: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.1103 Acc: 0.6187\n",
      "val Loss: 1.1628 Acc: 0.6065\n",
      "\n",
      "Cycle 1: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0941 Acc: 0.6173\n",
      "val Loss: 1.1428 Acc: 0.6085\n",
      "\n",
      "Cycle 1: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0863 Acc: 0.6194\n",
      "val Loss: 1.1432 Acc: 0.6146\n",
      "\n",
      "Cycle 2: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1961 Acc: 0.5911\n",
      "val Loss: 1.2026 Acc: 0.5680\n",
      "\n",
      "Cycle 2: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1822 Acc: 0.5829\n",
      "val Loss: 1.2041 Acc: 0.5822\n",
      "\n",
      "Cycle 2: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1565 Acc: 0.5929\n",
      "val Loss: 1.1924 Acc: 0.5984\n",
      "\n",
      "Cycle 2: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.1389 Acc: 0.5961\n",
      "val Loss: 1.2387 Acc: 0.5781\n",
      "\n",
      "Cycle 2: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.1242 Acc: 0.6047\n",
      "val Loss: 1.1669 Acc: 0.5984\n",
      "\n",
      "Cycle 2: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.0923 Acc: 0.6194\n",
      "val Loss: 1.1564 Acc: 0.6024\n",
      "\n",
      "Cycle 2: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0704 Acc: 0.6208\n",
      "val Loss: 1.1554 Acc: 0.5943\n",
      "\n",
      "Cycle 2: Epoch 7/9\n",
      "----------\n",
      "train Loss: 1.0534 Acc: 0.6362\n",
      "val Loss: 1.1345 Acc: 0.6126\n",
      "\n",
      "Cycle 2: Epoch 8/9\n",
      "----------\n",
      "train Loss: 1.0223 Acc: 0.6398\n",
      "val Loss: 1.1302 Acc: 0.6065\n",
      "\n",
      "Cycle 2: Epoch 9/9\n",
      "----------\n",
      "train Loss: 1.0181 Acc: 0.6362\n",
      "val Loss: 1.1267 Acc: 0.6105\n",
      "\n",
      "Cycle 3: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1396 Acc: 0.5936\n",
      "val Loss: 1.2191 Acc: 0.5903\n",
      "\n",
      "Cycle 3: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.1207 Acc: 0.6072\n",
      "val Loss: 1.2143 Acc: 0.5882\n",
      "\n",
      "Cycle 3: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.1207 Acc: 0.6069\n",
      "val Loss: 1.1780 Acc: 0.5842\n",
      "\n",
      "Cycle 3: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0981 Acc: 0.6072\n",
      "val Loss: 1.2028 Acc: 0.5903\n",
      "\n",
      "Cycle 3: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0579 Acc: 0.6205\n",
      "val Loss: 1.1810 Acc: 0.6024\n",
      "\n",
      "Cycle 3: Epoch 5/9\n",
      "----------\n",
      "train Loss: 1.0395 Acc: 0.6269\n",
      "val Loss: 1.1742 Acc: 0.5963\n",
      "\n",
      "Cycle 3: Epoch 6/9\n",
      "----------\n",
      "train Loss: 1.0009 Acc: 0.6477\n",
      "val Loss: 1.1611 Acc: 0.5943\n",
      "\n",
      "Cycle 3: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9875 Acc: 0.6434\n",
      "val Loss: 1.1368 Acc: 0.6045\n",
      "\n",
      "Cycle 3: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.9708 Acc: 0.6495\n",
      "val Loss: 1.1346 Acc: 0.5903\n",
      "\n",
      "Cycle 3: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.9617 Acc: 0.6531\n",
      "val Loss: 1.1374 Acc: 0.5943\n",
      "\n",
      "Cycle 4: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.1035 Acc: 0.6105\n",
      "val Loss: 1.2245 Acc: 0.5862\n",
      "\n",
      "Cycle 4: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0940 Acc: 0.6115\n",
      "val Loss: 1.1689 Acc: 0.5984\n",
      "\n",
      "Cycle 4: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0636 Acc: 0.6216\n",
      "val Loss: 1.1956 Acc: 0.5862\n",
      "\n",
      "Cycle 4: Epoch 3/9\n",
      "----------\n",
      "train Loss: 1.0438 Acc: 0.6327\n",
      "val Loss: 1.1844 Acc: 0.5963\n",
      "\n",
      "Cycle 4: Epoch 4/9\n",
      "----------\n",
      "train Loss: 1.0018 Acc: 0.6373\n",
      "val Loss: 1.1601 Acc: 0.5801\n",
      "\n",
      "Cycle 4: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9883 Acc: 0.6495\n",
      "val Loss: 1.1636 Acc: 0.5923\n",
      "\n",
      "Cycle 4: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.9541 Acc: 0.6581\n",
      "val Loss: 1.1474 Acc: 0.5822\n",
      "\n",
      "Cycle 4: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.9279 Acc: 0.6634\n",
      "val Loss: 1.1547 Acc: 0.5842\n",
      "\n",
      "Cycle 4: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.8905 Acc: 0.6853\n",
      "val Loss: 1.1508 Acc: 0.5822\n",
      "\n",
      "Cycle 4: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.8834 Acc: 0.6885\n",
      "val Loss: 1.1523 Acc: 0.5761\n",
      "\n",
      "Cycle 5: Epoch 0/9\n",
      "----------\n",
      "train Loss: 1.0554 Acc: 0.6276\n",
      "val Loss: 1.1813 Acc: 0.5903\n",
      "\n",
      "Cycle 5: Epoch 1/9\n",
      "----------\n",
      "train Loss: 1.0173 Acc: 0.6430\n",
      "val Loss: 1.2168 Acc: 0.5497\n",
      "\n",
      "Cycle 5: Epoch 2/9\n",
      "----------\n",
      "train Loss: 1.0000 Acc: 0.6480\n",
      "val Loss: 1.2302 Acc: 0.5781\n",
      "\n",
      "Cycle 5: Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.9649 Acc: 0.6516\n",
      "val Loss: 1.2561 Acc: 0.5781\n",
      "\n",
      "Cycle 5: Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.9551 Acc: 0.6549\n",
      "val Loss: 1.2316 Acc: 0.5538\n",
      "\n",
      "Cycle 5: Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.9079 Acc: 0.6717\n",
      "val Loss: 1.1990 Acc: 0.5619\n",
      "\n",
      "Cycle 5: Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.8699 Acc: 0.6810\n",
      "val Loss: 1.2191 Acc: 0.5598\n",
      "\n",
      "Cycle 5: Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.8252 Acc: 0.6989\n",
      "val Loss: 1.2095 Acc: 0.5781\n",
      "\n",
      "Cycle 5: Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.7982 Acc: 0.7125\n",
      "val Loss: 1.2063 Acc: 0.5659\n",
      "\n",
      "Cycle 5: Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.8023 Acc: 0.7118\n",
      "val Loss: 1.2021 Acc: 0.5680\n",
      "\n",
      "Training complete in 9m 45s\n",
      "Ensemble Loss : 1.125766, Best val Loss: 1.126698\n"
     ]
    }
   ],
   "source": [
    "models_arr = []\n",
    "fold = 0\n",
    "for train_index, val_index in sss.split(areas[gts > -1], gts[gts > -1]):\n",
    "    print(fold)\n",
    "    fold += 1\n",
    "    image_datasets = {'train': ICLRDataset(imgs, areas, gts, field_masks, 'train', train_index),\n",
    "                      'val': ICLRDataset(imgs, areas, gts, field_masks, 'val', val_index),\n",
    "                      'test': ICLRDataset(imgs, areas, gts, field_masks, 'test', None)}\n",
    "\n",
    "    dataloaders = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=16, shuffle=True, num_workers=16),\n",
    "                   'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=16, shuffle=False, num_workers=16)}\n",
    "\n",
    "    model_ft = ConvGRUNet(imgs.shape[2]-1)\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    dataset_sizes = {x:len(image_datasets[x]) for x in ['train', 'val']}\n",
    "    \n",
    "    #train a model on this data split using snapshot ensemble\n",
    "    model_ft_arr, _, _ = train_model_snapshot(model_ft, criterion, 0.008, dataloaders, dataset_sizes, device,\n",
    "                           num_cycles=6, num_epochs_per_cycle=10)\n",
    "    models_arr.extend(model_ft_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=16,shuffle=False, num_workers=4)\n",
    "res = test(models_arr, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_path = 'SampleSubmission.csv'\n",
    "test_fields_arr = np.array(fields_arr)[gts == -1]\n",
    "sub = pd.read_csv(sample_submission_path)\n",
    "sub['Field_ID'] = test_fields_arr.tolist()\n",
    "for i in range(res.shape[1]):\n",
    "    sub['Crop_ID_%d'%(i+1)] = res[:,i].tolist()\n",
    "sub.to_csv('sub.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You should achieve 1.114 on the leaderboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
